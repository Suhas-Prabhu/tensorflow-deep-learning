{"cells":[{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2021-11-09T16:11:49.992531Z","iopub.status.busy":"2021-11-09T16:11:49.992155Z","iopub.status.idle":"2021-11-09T16:11:50.025005Z","shell.execute_reply":"2021-11-09T16:11:50.023575Z","shell.execute_reply.started":"2021-11-09T16:11:49.992433Z"}},"source":["To improve our model(s), we could spend a while trying different configurations, adding more layers, changing the learning rate, adjusting the number of neurons per layer and more.\n","\n","However, doing this is very time consuming.\n","\n","Luckily, there's a technique we can use to save time.\n","\n","It's called **transfer learning**, in other words, taking the patterns (also called weights) another model has learned from another problem and using them for our own problem.\n","\n","There are two main benefits to using transfer learning:\n","1. Can leverage an existing neural network architecture proven to work on problems similar to our own.\n","2. Can leverage a working neural network architecture which has **already learned** patterns on similar data to our own. This often results in achieving great results with less custom data.\n","\n","What this means is, instead of hand-crafting our own neural network architectures or building them from scratch, we can utilise models which have worked for others.\n","\n","And instead of training our own models from scratch on our own datasets, we can take the patterns a model has learned from datasets such as [ImageNet](http://www.image-net.org/) (millions of images of different objects) and use them as the foundation of our own. Doing this often leads to getting great results with less data."]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2021-11-10T14:13:40.173933Z","iopub.status.busy":"2021-11-10T14:13:40.173632Z","iopub.status.idle":"2021-11-10T14:13:40.904444Z","shell.execute_reply":"2021-11-10T14:13:40.903595Z","shell.execute_reply.started":"2021-11-10T14:13:40.173851Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Sat Jan 15 19:26:01 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.86       Driver Version: 470.86       CUDA Version: 11.4     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\n","| N/A   45C    P0    12W /  N/A |    519MiB /  3911MiB |     11%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|    0   N/A  N/A       859      G   /usr/lib/xorg/Xorg                 70MiB |\n","|    0   N/A  N/A      1575      G   /usr/lib/xorg/Xorg                169MiB |\n","|    0   N/A  N/A      1745      G   /usr/bin/gnome-shell              129MiB |\n","|    0   N/A  N/A      2103      G   ...AAAAAAAAA= --shared-files       40MiB |\n","|    0   N/A  N/A      2442      G   ...AAAAAAAAA= --shared-files       95MiB |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["# Are we using a GPU?\n","!nvidia-smi"]},{"cell_type":"markdown","metadata":{},"source":["## Transfer leanring with TensorFlow Hub: Getting great results with 10% of the data\n","\n","For many of the problems you'll want to use deep learning for, chances are, a working model already exists.\n","\n","And the good news is, you can access many of them on TensorFlow Hub.\n","\n","[TensorFlow Hub](https://tfhub.dev/) is a repository for existing model components. It makes it so you can import and use a fully trained model with as little as a URL.\n","\n","To do so, what if I told you we could get much of the same results (or better) than our best model has gotten so far with only 10% of the original data, in other words, 10x less data.\n","\n","This seems counterintuitive right?\n","\n","Wouldn't you think more examples of what a picture of food looked like led to better results?\n","\n","And you'd be right if you thought so, generally, more data leads to better results.\n","\n","However, what if you didn't have more data? What if instead of 750 images per class, you had 75 images per class?\n","\n","Collecting 675 more images of a certain class could take a long time.\n","\n","So this is where another major benefit of transfer learning comes in.\n","\n","**Transfer learning often allows you to get great results with less data.**\n","\n","Let's download a subset of the data we've been using, namely 10% of the training data from the `10_food_classes` dataset and use it to train a food image classifier on.\n","\n","![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/04-transfer-learning-feature-extraction.png)\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2021-11-10T14:13:40.906961Z","iopub.status.busy":"2021-11-10T14:13:40.906589Z","iopub.status.idle":"2021-11-10T14:13:46.222391Z","shell.execute_reply":"2021-11-10T14:13:46.221479Z","shell.execute_reply.started":"2021-11-10T14:13:40.906922Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["--2022-01-15 18:42:16--  https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.195.208, 142.250.195.240, 142.250.196.16, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.195.208|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 168546183 (161M) [application/zip]\n","Saving to: ‘10_food_classes_10_percent.zip.1’\n","\n","ses_10_percent.zip.  47%[========>           ]  75.77M  6.41MB/s    eta 18s    ^C\n"]}],"source":["import zipfile\n","\n","# Download data\n","!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n","\n","zip_ref = zipfile.ZipFile(\"10_food_classes_10_percent.zip\",\"r\")\n","zip_ref.extractall()\n","zip_ref.close()"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2021-11-10T14:13:46.224434Z","iopub.status.busy":"2021-11-10T14:13:46.224116Z","iopub.status.idle":"2021-11-10T14:13:46.245868Z","shell.execute_reply":"2021-11-10T14:13:46.244281Z","shell.execute_reply.started":"2021-11-10T14:13:46.224381Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["There are 2 directories and 0 images in 10_food_classes_10_percent\n","There are 10 directories and 0 images in 10_food_classes_10_percent/test\n","There are 0 directories and 250 images in 10_food_classes_10_percent/test/chicken_wings\n","There are 0 directories and 250 images in 10_food_classes_10_percent/test/ramen\n","There are 0 directories and 250 images in 10_food_classes_10_percent/test/grilled_salmon\n","There are 0 directories and 250 images in 10_food_classes_10_percent/test/pizza\n","There are 0 directories and 250 images in 10_food_classes_10_percent/test/ice_cream\n","There are 0 directories and 250 images in 10_food_classes_10_percent/test/chicken_curry\n","There are 0 directories and 250 images in 10_food_classes_10_percent/test/steak\n","There are 0 directories and 250 images in 10_food_classes_10_percent/test/hamburger\n","There are 0 directories and 250 images in 10_food_classes_10_percent/test/fried_rice\n","There are 0 directories and 250 images in 10_food_classes_10_percent/test/sushi\n","There are 10 directories and 0 images in 10_food_classes_10_percent/train\n","There are 0 directories and 75 images in 10_food_classes_10_percent/train/chicken_wings\n","There are 0 directories and 75 images in 10_food_classes_10_percent/train/ramen\n","There are 0 directories and 75 images in 10_food_classes_10_percent/train/grilled_salmon\n","There are 0 directories and 75 images in 10_food_classes_10_percent/train/pizza\n","There are 0 directories and 75 images in 10_food_classes_10_percent/train/ice_cream\n","There are 0 directories and 75 images in 10_food_classes_10_percent/train/chicken_curry\n","There are 0 directories and 75 images in 10_food_classes_10_percent/train/steak\n","There are 0 directories and 75 images in 10_food_classes_10_percent/train/hamburger\n","There are 0 directories and 75 images in 10_food_classes_10_percent/train/fried_rice\n","There are 0 directories and 75 images in 10_food_classes_10_percent/train/sushi\n"]}],"source":["import os\n","for dirpath, dirnames, filenames in os.walk(\"10_food_classes_10_percent\"):\n","    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in {dirpath}\")"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2021-11-10T14:13:46.248610Z","iopub.status.busy":"2021-11-10T14:13:46.247866Z","iopub.status.idle":"2021-11-10T14:13:51.330681Z","shell.execute_reply":"2021-11-10T14:13:51.329960Z","shell.execute_reply.started":"2021-11-10T14:13:46.248567Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 750 images belonging to 10 classes.\n","Found 2500 images belonging to 10 classes.\n"]}],"source":["# lets process the data before we proceed\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","Image_shape =(224,224)\n","Batch_Size = 32\n","\n","train_dir = \"10_food_classes_10_percent/train/\"\n","test_dir =\"10_food_classes_10_percent/test/\"\n","\n","train_gen = ImageDataGenerator(rescale = 1/255.)\n","test_gen = ImageDataGenerator(rescale = 1/255.)\n","\n","train_data_10 = train_gen.flow_from_directory(train_dir,\n","                                             target_size = Image_shape,\n","                                             batch_size = Batch_Size,\n","                                             class_mode = 'categorical')\n","test_data_10 = train_gen.flow_from_directory(test_dir,\n","                                             target_size = Image_shape,\n","                                             batch_size = Batch_Size,\n","                                             class_mode = 'categorical')"]},{"cell_type":"markdown","metadata":{},"source":["## Setting up callbacks (things to run whilst our model trains)\n","\n","Before we build a model, there's an important concept we're going to get familiar with because it's going to play a key role in our future model building experiments.\n","\n","And that concept is **callbacks**.\n","\n","[Callbacks](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks) are extra functionality you can add to your models to be performed during or after training. Some of the most popular callbacks include:\n","* [**Experiment tracking with TensorBoard**](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard) - log the performance of multiple models and then view and compare these models in a visual way on [TensorBoard](https://www.tensorflow.org/tensorboard) (a dashboard for inspecting neural network parameters). Helpful to compare the results of different models on your data.\n","* [**Model checkpointing**](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint) - save your model as it trains so you can stop training if needed and come back to continue off where you left. Helpful if training takes a long time and can't be done in one sitting.\n","* [**Early stopping**](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) - leave your model training for an arbitrary amount of time and have it stop training automatically when it ceases to improve. Helpful when you've got a large dataset and don't know how long training will take.\n","\n","We'll explore each of these overtime but for this notebook, we'll see how the TensorBoard callback can be used.\n","\n","The TensorBoard callback can be accessed using [`tf.keras.callbacks.TensorBoard()`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard). \n","\n","Its main functionality is saving a model's training performance metrics to a specified `log_dir`.\n","\n","By default, logs are recorded every epoch using the `update_freq='epoch'` parameter. This is a good default since tracking model performance too often can slow down model training.\n","\n","To track our modelling experiments using TensorBoard, let's create a function which creates a TensorBoard callback for us."]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2021-11-10T14:34:02.328241Z","iopub.status.busy":"2021-11-10T14:34:02.327989Z","iopub.status.idle":"2021-11-10T14:34:02.333207Z","shell.execute_reply":"2021-11-10T14:34:02.332551Z","shell.execute_reply.started":"2021-11-10T14:34:02.328214Z"},"trusted":true},"outputs":[],"source":["# create tensorflow callback function\n","import datetime\n","\n","def create_dashboard(dir_name,exp_name):\n","    log_dir = dir_name+\"/\"+exp_name+\"/\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n","        log_dir = log_dir\n","    )\n","    print(f\"Saving Tensorboard log files to: {log_dir}\")\n","    return tensorboard_callback"]},{"cell_type":"markdown","metadata":{},"source":["Because you're likely to run multiple experiments, it's a good idea to be able to track them in some way.\n","\n","In our case, our function saves a model's performance logs to a directory named [dir_name]/[experiment_name]/[current_timestamp], where:\n","\n","dir_name is the overall logs directory\n","experiment_name is the particular experiment\n","current_timestamp is the time the experiment started based on Python's datetime.datetime().now()\n","\n","In the past we've used TensorFlow to create our own models layer by layer from scratch.\n","\n","Now we're going to do a similar process, except the majority of our model's layers are going to come from [TensorFlow Hub](https://tfhub.dev/).\n","\n","In fact, we're going to use two models from TensorFlow Hub:\n","1. [ResNetV2](https://arxiv.org/abs/1603.05027) -  a state of the art computer vision model architecture from 2016.\n","2. [EfficientNet](https://arxiv.org/abs/1905.11946) - a state of the art computer vision architecture from 2019.\n","\n","State of the art means that at some point, both of these models have achieved the lowest error rate on [ImageNet (ILSVRC-2012-CLS)](http://www.image-net.org/), the gold standard of computer vision benchmarks.\n","\n","You might be wondering, how do you find these models on TensorFlow Hub?\n","\n","Here are the steps I took:\n","\n","1. Go to [tfhub.dev](https://tfhub.dev/).\n","2. Choose your problem domain, e.g. \"Image\" (we're using food images).\n","3. Select your TF version, which in our case is TF2.\n","4. Remove all \"Problem domanin\" filters except for the problem you're working on. \n","  * **Note:** \"Image feature vector\" can be used alongside almost any problem, we'll get to this soon.\n","5. The models listed are all models which could potentially be used for your problem.\n","\n","You can see a list of state of the art models on [paperswithcode.com](https://www.paperswithcode.com), a resource for collecting the latest in deep learning paper results which have code implementations for the findings they report.\n","\n","Since we're working with images, our target are the [models which perform best on ImageNet](https://paperswithcode.com/sota/image-classification-on-imagenet).\n","\n","You'll probably find not all of the model architectures listed on paperswithcode appear on TensorFlow Hub. And this is okay, we can still use what's available.\n","\n","To find our models, let's narrow down our search using the Architecture tab.\n","\n","6. Select the Architecture tab on TensorFlow Hub and you'll see a dropdown menu of architecture names appear. \n","  * The rule of thumb here is generally, names with larger numbers means better performing models. For example, EfficientNetB4 performs better than EfficientNetB0.\n","    * However, the tradeoff with larger numbers can mean they take longer to compute. \n","7. Select EfficientNetB0 and you should see [something like the following](https://tfhub.dev/s?module-type=image-classification,image-feature-vector&network-architecture=efficientnet-b0&tf-version=tf2):\n","![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/04-tensorflow-hub-efficientnetb0.png)\n","8. Clicking the one titled \"[efficientnet/b0/feature-vector](https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1)\" brings us to a page with a button that says \"Copy URL\". That URL is what we can use to harness the power of EfficientNetB0.\n","  * Copying the URL should give you something like this: https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\n","\n","This is where the differnet types of transfer learning come into play, as is, feature extraction and fine-tuning.\n","\n","1. **\"As is\" transfer learning** is when you take a pretrained model as it is and apply it to your task without any changes. \n","\n","  * For example, many computer vision models are pretrained on the ImageNet dataset which contains 1000 different classes of images. This means passing a single image to this model will produce 1000 different prediction probability values (1 for each class). \n","\n","    * This is helpful if you have 1000 classes of image you'd like to classify and they're all the same as the ImageNet classes, however, it's not helpful if you want to classify only a small subset of classes (such as 10 different kinds of food). Model's with `\"/classification\"` in their name on TensorFlow Hub provide this kind of functionality.\n","\n","2. **Feature extraction transfer learning** is when you take the underlying patterns (also called weights) a pretrained model has learned and adjust its outputs to be more suited to your problem. \n","\n","  * For example, say the pretrained model you were using had 236 different layers (EfficientNetB0 has 236 layers), but the top layer outputs 1000 classes because it was pretrained on ImageNet. To adjust this to your own problem, you might remove the original activation layer and replace it with your own but with the right number of output classes. The important part here is that **only the top few layers become trainable, the rest remain frozen**. \n","\n","    * This way all the underlying patterns remain in the rest of the layers and you can utilise them for your own problem. This kind of transfer learning is very helpful when your data is similar to the data a model has been pretrained on.\n","\n","3. **Fine-tuning transfer learning** is when you take the underlying patterns (also called weights) of a pretrained model and adjust (fine-tune) them to your own problem. \n","\n","    * This usually means training **some, many or all** of the layers in the pretrained model. This is useful when you've got a large dataset (e.g. 100+ images per class) where your data is slightly different to the data the original model was trained on.\n","\n","A common workflow is to \"freeze\" all of the learned patterns in the bottom layers of a pretrained model so they're untrainable. And then train the top 2-3 layers of so the pretrained model can adjust its outputs to your custom data (**feature extraction**).\n","\n","After you've trained the top 2-3 layers, you can then gradually \"unfreeze\" more and more layers and run the training process on your own data to further **fine-tune** the pretrained model.\n","\n","The lower a layer is in a computer vision model as in, the closer it is to the input layer, the larger the features it learn. For example, a bottom layer in a computer vision model to identify images of cats or dogs might learn the outline of legs, where as, layers closer to the output might learn the shape of teeth. Often, you'll want the larger features (learned patterns are also called features) to remain, since these are similar for both animals, where as, the differences remain in the more fine-grained features.\n","\n","![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/04-different-kinds-of-transfer-learning.png)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2021-11-10T14:14:52.568911Z","iopub.status.busy":"2021-11-10T14:14:52.568543Z","iopub.status.idle":"2021-11-10T14:14:52.994172Z","shell.execute_reply":"2021-11-10T14:14:52.993159Z","shell.execute_reply.started":"2021-11-10T14:14:52.568867Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","import tensorflow_hub as hub\n","from tensorflow.keras import layers"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2021-11-10T14:20:10.466273Z","iopub.status.busy":"2021-11-10T14:20:10.465529Z","iopub.status.idle":"2021-11-10T14:20:10.469945Z","shell.execute_reply":"2021-11-10T14:20:10.469246Z","shell.execute_reply.started":"2021-11-10T14:20:10.466234Z"},"trusted":true},"outputs":[],"source":["resnet_url = \"https://tfhub.dev/google/imagenet/resnet_v1_50/feature_vector/5\"\n","efficientnet_url = \"https://tfhub.dev/google/efficientnet/b0/feature-vector/1\"\n"]},{"cell_type":"markdown","metadata":{},"source":["These URLs link to a saved pretrained model on TensorFlow Hub.\n","\n","When we use them in our model, the model will automatically be downloaded for us to use.\n","\n","To do this, we can use the KerasLayer() model inside the TensorFlow hub library.\n","\n","Since we're going to be comparing two models, to save ourselves code, we'll create a function create_model(). This function will take a model's TensorFlow Hub URL, instatiate a Keras Sequential model with the appropriate number of output layers and return the model."]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2021-11-10T14:30:22.143138Z","iopub.status.busy":"2021-11-10T14:30:22.142689Z","iopub.status.idle":"2021-11-10T14:30:22.147937Z","shell.execute_reply":"2021-11-10T14:30:22.147277Z","shell.execute_reply.started":"2021-11-10T14:30:22.143100Z"},"trusted":true},"outputs":[],"source":["def create_model(model_url,num_classes=10):\n","    \"\"\"\n","    Takes a tensorflow hub url and creates a sequential model with it\n","    \"\"\"\n","    feat_extract_layer = hub.KerasLayer(model_url,\n","                                       trainable = False, #freeze the underlying patterns\n","                                       name ='feature_extraction_layer',\n","                                       input_shape = Image_shape+(3,))\n","    # create our own model\n","    model = tf.keras.Sequential([\n","        feat_extract_layer,\n","        layers.Dense(num_classes,activation ='softmax',name ='output_layer')\n","    ])\n","    \n","    return model"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2021-11-10T14:30:22.746365Z","iopub.status.busy":"2021-11-10T14:30:22.745670Z","iopub.status.idle":"2021-11-10T14:30:31.084945Z","shell.execute_reply":"2021-11-10T14:30:31.084086Z","shell.execute_reply.started":"2021-11-10T14:30:22.746294Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-01-15 19:26:33.980019: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2022-01-15 19:26:34.008918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-01-15 19:26:34.009164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1650 Ti computeCapability: 7.5\n","coreClock: 1.485GHz coreCount: 16 deviceMemorySize: 3.82GiB deviceMemoryBandwidth: 178.84GiB/s\n","2022-01-15 19:26:34.010624: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2022-01-15 19:26:34.032729: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2022-01-15 19:26:34.045500: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2022-01-15 19:26:34.049313: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2022-01-15 19:26:34.073318: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2022-01-15 19:26:34.077618: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2022-01-15 19:26:34.126424: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2022-01-15 19:26:34.126645: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-01-15 19:26:34.127375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-01-15 19:26:34.127753: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2022-01-15 19:26:34.128387: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n","2022-01-15 19:26:34.161058: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2400000000 Hz\n","2022-01-15 19:26:34.161816: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7efaf8000b60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2022-01-15 19:26:34.161845: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2022-01-15 19:26:34.241275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-01-15 19:26:34.241645: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563eddffd1f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2022-01-15 19:26:34.241659: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1650 Ti, Compute Capability 7.5\n","2022-01-15 19:26:34.242224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-01-15 19:26:34.242514: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1650 Ti computeCapability: 7.5\n","coreClock: 1.485GHz coreCount: 16 deviceMemorySize: 3.82GiB deviceMemoryBandwidth: 178.84GiB/s\n","2022-01-15 19:26:34.242592: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2022-01-15 19:26:34.242605: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2022-01-15 19:26:34.242616: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2022-01-15 19:26:34.242626: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2022-01-15 19:26:34.242637: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2022-01-15 19:26:34.242645: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2022-01-15 19:26:34.242656: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2022-01-15 19:26:34.242719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-01-15 19:26:34.242933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-01-15 19:26:34.243235: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2022-01-15 19:26:34.243522: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2022-01-15 19:26:34.244569: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2022-01-15 19:26:34.244579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n","2022-01-15 19:26:34.244583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n","2022-01-15 19:26:34.244919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-01-15 19:26:34.245116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-01-15 19:26:34.245292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3062 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1650 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\n"]}],"source":["# create the model\n","resnet_model = create_model(resnet_url,num_classes=train_data_10.num_classes)\n","\n","resnet_model.compile(loss = \"categorical_crossentropy\",\n","                    optimizer = tf.keras.optimizers.Adam(),\n","                    metrics =['accuracy'])\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["import os\n","os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = \"False\""]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2021-11-10T14:34:07.881802Z","iopub.status.busy":"2021-11-10T14:34:07.881469Z","iopub.status.idle":"2021-11-10T14:35:33.308069Z","shell.execute_reply":"2021-11-10T14:35:33.307368Z","shell.execute_reply.started":"2021-11-10T14:34:07.881766Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-01-15 19:27:00.550729: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session started.\n","2022-01-15 19:27:00.551416: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1363] Profiler found 1 GPUs\n","2022-01-15 19:27:00.556772: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcupti.so.10.1\n","2022-01-15 19:27:00.657696: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1408] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI_ERROR_INSUFFICIENT_PRIVILEGES\n","2022-01-15 19:27:00.658155: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1447] function cupti_interface_->ActivityRegisterCallbacks( AllocCuptiActivityBuffer, FreeCuptiActivityBuffer)failed with error CUPTI_ERROR_INSUFFICIENT_PRIVILEGES\n","2022-01-15 19:27:00.658803: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1430] function cupti_interface_->EnableCallback( 0 , subscriber_, CUPTI_CB_DOMAIN_DRIVER_API, cbid)failed with error CUPTI_ERROR_INVALID_PARAMETER\n"]},{"name":"stdout","output_type":"stream","text":["Saving Tensorboard log files to: tensorflow_hub/resnet/20220115-192700\n","Epoch 1/5\n"]},{"name":"stderr","output_type":"stream","text":["2022-01-15 19:27:03.232886: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 341 in the outer inference context.\n","2022-01-15 19:27:03.232950: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 312 in the outer inference context.\n","2022-01-15 19:27:03.232974: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 262 in the outer inference context.\n","2022-01-15 19:27:03.233069: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 395 in the outer inference context.\n","2022-01-15 19:27:03.233108: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 302 in the outer inference context.\n","2022-01-15 19:27:03.233140: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 232 in the outer inference context.\n","2022-01-15 19:27:03.233182: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 362 in the outer inference context.\n","2022-01-15 19:27:03.233206: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 202 in the outer inference context.\n","2022-01-15 19:27:03.233216: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 405 in the outer inference context.\n","2022-01-15 19:27:03.233247: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 332 in the outer inference context.\n","2022-01-15 19:27:03.233287: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 322 in the outer inference context.\n","2022-01-15 19:27:03.233303: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 383 in the outer inference context.\n","2022-01-15 19:27:03.233339: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 373 in the outer inference context.\n","2022-01-15 19:27:03.233356: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 352 in the outer inference context.\n","2022-01-15 19:27:03.233410: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 292 in the outer inference context.\n","2022-01-15 19:27:03.233489: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 282 in the outer inference context.\n","2022-01-15 19:27:03.233525: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 252 in the outer inference context.\n","2022-01-15 19:27:03.233567: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 242 in the outer inference context.\n","2022-01-15 19:27:03.233582: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 222 in the outer inference context.\n","2022-01-15 19:27:03.233684: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 599 in the outer inference context.\n","2022-01-15 19:27:03.233713: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 180 in the outer inference context.\n","2022-01-15 19:27:03.233739: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 109 in the outer inference context.\n","2022-01-15 19:27:03.233793: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 148 in the outer inference context.\n","2022-01-15 19:27:03.233853: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 169 in the outer inference context.\n","2022-01-15 19:27:03.233888: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 190 in the outer inference context.\n","2022-01-15 19:27:03.233907: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 159 in the outer inference context.\n","2022-01-15 19:27:03.233922: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 139 in the outer inference context.\n","2022-01-15 19:27:03.233948: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 79 in the outer inference context.\n","2022-01-15 19:27:03.233983: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 99 in the outer inference context.\n","2022-01-15 19:27:03.234011: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 129 in the outer inference context.\n","2022-01-15 19:27:03.234082: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 119 in the outer inference context.\n","2022-01-15 19:27:03.234164: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 89 in the outer inference context.\n","2022-01-15 19:27:03.234324: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 568 in the outer inference context.\n","2022-01-15 19:27:03.234378: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 425 in the outer inference context.\n","2022-01-15 19:27:03.234394: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 558 in the outer inference context.\n","2022-01-15 19:27:03.234418: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 528 in the outer inference context.\n","2022-01-15 19:27:03.234435: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 495 in the outer inference context.\n","2022-01-15 19:27:03.234450: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 465 in the outer inference context.\n","2022-01-15 19:27:03.234470: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 589 in the outer inference context.\n","2022-01-15 19:27:03.234525: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 609 in the outer inference context.\n","2022-01-15 19:27:03.234562: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 415 in the outer inference context.\n","2022-01-15 19:27:03.234629: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 485 in the outer inference context.\n","2022-01-15 19:27:03.234681: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 435 in the outer inference context.\n","2022-01-15 19:27:03.234704: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 455 in the outer inference context.\n","2022-01-15 19:27:03.234730: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 548 in the outer inference context.\n","2022-01-15 19:27:03.234798: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 577 in the outer inference context.\n","2022-01-15 19:27:03.234820: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 538 in the outer inference context.\n","2022-01-15 19:27:03.234836: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 516 in the outer inference context.\n","2022-01-15 19:27:03.234863: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 445 in the outer inference context.\n","2022-01-15 19:27:03.234909: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 474 in the outer inference context.\n","2022-01-15 19:27:03.234947: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 506 in the outer inference context.\n","2022-01-15 19:27:03.234960: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 212 in the outer inference context.\n","2022-01-15 19:27:03.234997: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 272 in the outer inference context.\n","2022-01-15 19:27:03.695021: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 139 in the outer inference context.\n","2022-01-15 19:27:03.695264: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 109 in the outer inference context.\n","2022-01-15 19:27:03.695333: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 169 in the outer inference context.\n","2022-01-15 19:27:03.695553: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 129 in the outer inference context.\n","2022-01-15 19:27:03.695684: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 89 in the outer inference context.\n","2022-01-15 19:27:03.695752: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 79 in the outer inference context.\n","2022-01-15 19:27:03.695871: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 119 in the outer inference context.\n","2022-01-15 19:27:03.695945: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 159 in the outer inference context.\n","2022-01-15 19:27:03.695957: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 99 in the outer inference context.\n","2022-01-15 19:27:03.695992: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 190 in the outer inference context.\n","2022-01-15 19:27:03.696031: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 148 in the outer inference context.\n","2022-01-15 19:27:03.696160: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 506 in the outer inference context.\n","2022-01-15 19:27:03.696190: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 474 in the outer inference context.\n","2022-01-15 19:27:03.696219: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 599 in the outer inference context.\n","2022-01-15 19:27:03.696239: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 577 in the outer inference context.\n","2022-01-15 19:27:03.696268: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 538 in the outer inference context.\n","2022-01-15 19:27:03.696383: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 465 in the outer inference context.\n","2022-01-15 19:27:03.696410: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 589 in the outer inference context.\n","2022-01-15 19:27:03.696428: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 568 in the outer inference context.\n","2022-01-15 19:27:03.696460: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 528 in the outer inference context.\n","2022-01-15 19:27:03.696611: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 558 in the outer inference context.\n","2022-01-15 19:27:03.696663: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 202 in the outer inference context.\n","2022-01-15 19:27:03.696683: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 455 in the outer inference context.\n","2022-01-15 19:27:03.696734: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 495 in the outer inference context.\n","2022-01-15 19:27:03.696802: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 548 in the outer inference context.\n","2022-01-15 19:27:03.696863: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 415 in the outer inference context.\n","2022-01-15 19:27:03.696899: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 373 in the outer inference context.\n","2022-01-15 19:27:03.696933: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 485 in the outer inference context.\n","2022-01-15 19:27:03.696957: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 435 in the outer inference context.\n","2022-01-15 19:27:03.697047: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 395 in the outer inference context.\n","2022-01-15 19:27:03.697159: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 425 in the outer inference context.\n","2022-01-15 19:27:03.697266: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 516 in the outer inference context.\n","2022-01-15 19:27:03.697337: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 272 in the outer inference context.\n","2022-01-15 19:27:03.697378: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 362 in the outer inference context.\n","2022-01-15 19:27:03.697423: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 445 in the outer inference context.\n","2022-01-15 19:27:03.697477: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 341 in the outer inference context.\n","2022-01-15 19:27:03.697549: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 332 in the outer inference context.\n","2022-01-15 19:27:03.697581: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 383 in the outer inference context.\n","2022-01-15 19:27:03.697629: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 405 in the outer inference context.\n","2022-01-15 19:27:03.697652: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 262 in the outer inference context.\n","2022-01-15 19:27:03.697665: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 352 in the outer inference context.\n","2022-01-15 19:27:03.697694: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 302 in the outer inference context.\n","2022-01-15 19:27:03.697738: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 222 in the outer inference context.\n","2022-01-15 19:27:03.697759: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 292 in the outer inference context.\n","2022-01-15 19:27:03.697788: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 322 in the outer inference context.\n","2022-01-15 19:27:03.697821: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 252 in the outer inference context.\n","2022-01-15 19:27:03.697884: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 212 in the outer inference context.\n","2022-01-15 19:27:03.697919: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 282 in the outer inference context.\n","2022-01-15 19:27:03.697990: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 242 in the outer inference context.\n","2022-01-15 19:27:03.698005: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 312 in the outer inference context.\n","2022-01-15 19:27:03.698069: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 232 in the outer inference context.\n","2022-01-15 19:27:03.698128: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 180 in the outer inference context.\n","2022-01-15 19:27:03.698170: W tensorflow/core/common_runtime/shape_refiner.cc:88] Function instantiation has undefined input shape at index: 609 in the outer inference context.\n","2022-01-15 19:27:05.182020: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2022-01-15 19:27:05.784098: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2022-01-15 19:27:05.996103: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\n","2022-01-15 19:27:06.000460: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\n"]},{"ename":"UnknownError","evalue":" Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node sequential/feature_extraction_layer/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/predict/resnet_v1_50/conv1/Conv2D}}]] [Op:__inference_train_function_20091]\n\nFunction call stack:\ntrain_function\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_2723/3042616893.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m resnet_history = resnet_model.fit(train_data_10,\n\u001b[0m\u001b[1;32m      2\u001b[0m                                  \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                  \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                  \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_data_10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                  \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/personal/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/personal/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/personal/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/personal/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/personal/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/personal/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \"\"\"\n\u001b[0;32m-> 1661\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1662\u001b[0m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n","\u001b[0;32m~/anaconda3/envs/personal/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n","\u001b[0;32m~/anaconda3/envs/personal/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    591\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/personal/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUnknownError\u001b[0m:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node sequential/feature_extraction_layer/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/predict/resnet_v1_50/conv1/Conv2D}}]] [Op:__inference_train_function_20091]\n\nFunction call stack:\ntrain_function\n"]}],"source":["resnet_history = resnet_model.fit(train_data_10,\n","                                 epochs =5,\n","                                 steps_per_epoch=len(train_data_10),\n","                                 validation_data=test_data_10,\n","                                 validation_steps=len(test_data_10),\n","                                 callbacks=[create_dashboard(dir_name=\"tensorflow_hub\",exp_name=\"resnet\")])"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2021-11-10T14:41:07.262344Z","iopub.status.busy":"2021-11-10T14:41:07.262064Z","iopub.status.idle":"2021-11-10T14:41:07.269444Z","shell.execute_reply":"2021-11-10T14:41:07.268754Z","shell.execute_reply.started":"2021-11-10T14:41:07.262298Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","def plot_curves(history):\n","    \"\"\"\n","    return seperate loss curves and accuracy curves for train and validation metrics\n","    \"\"\"\n","    loss = history.history['loss']\n","    val_loss = history.history['val_loss']\n","    acc = history.history['accuracy']\n","    val_acc = history.history['val_accuracy']\n","    epochs = range(len(loss))\n","    plt.plot(epochs,loss,label ='train_loss')\n","    plt.plot(epochs,val_loss,label='val_loss')\n","    plt.title(\"loss\")\n","    plt.xlabel('epochs')\n","    plt.legend()\n","    \n","    plt.figure()\n","    plt.plot(epochs,acc,label ='train_acc')\n","    plt.plot(epochs,val_acc,label='val_acc')\n","    plt.title(\"Accuracy\")\n","    plt.xlabel('epochs')\n","    plt.legend()"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2021-11-10T14:41:18.185891Z","iopub.status.busy":"2021-11-10T14:41:18.185220Z","iopub.status.idle":"2021-11-10T14:41:18.653200Z","shell.execute_reply":"2021-11-10T14:41:18.652546Z","shell.execute_reply.started":"2021-11-10T14:41:18.185852Z"},"trusted":true},"outputs":[],"source":["plot_curves(resnet_history)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2021-11-10T14:41:42.611488Z","iopub.status.busy":"2021-11-10T14:41:42.610686Z","iopub.status.idle":"2021-11-10T14:41:42.626577Z","shell.execute_reply":"2021-11-10T14:41:42.625895Z","shell.execute_reply.started":"2021-11-10T14:41:42.611423Z"},"trusted":true},"outputs":[],"source":["resnet_model.summary()"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2021-11-10T14:46:28.339718Z","iopub.status.busy":"2021-11-10T14:46:28.339461Z","iopub.status.idle":"2021-11-10T14:47:47.852581Z","shell.execute_reply":"2021-11-10T14:47:47.851618Z","shell.execute_reply.started":"2021-11-10T14:46:28.339689Z"},"trusted":true},"outputs":[],"source":["eff_model = create_model(efficientnet_url,train_data_10.num_classes)\n","eff_model.compile(loss ='categorical_crossentropy',\n","                 optimizer =tf.keras.optimizers.Adam(),\n","                 metrics =[\"accuracy\"])\n","eff_hist = eff_model.fit(train_data_10,\n","                            epochs =5,\n","                        steps_per_epoch = len(train_data_10),\n","                        validation_data = test_data_10,\n","                        validation_steps = len(test_data_10),\n","                        callbacks = [create_dashboard(\"tensorflow_hub\",\"efficientnetB0\")])"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2021-11-10T14:56:03.378608Z","iopub.status.busy":"2021-11-10T14:56:03.377903Z","iopub.status.idle":"2021-11-10T14:56:03.836858Z","shell.execute_reply":"2021-11-10T14:56:03.835539Z","shell.execute_reply.started":"2021-11-10T14:56:03.378571Z"},"trusted":true},"outputs":[],"source":["plot_curves(eff_hist)\n","eff_model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["## Comparing models using TensorBoard\n","\n","Alright, even though we've already compared the performance of our two models by looking at the accuracy scores. But what if you had more than two models? \n","\n","That's where an experiment tracking tool like [TensorBoard](https://www.tensorflow.org/tensorboard) (preinstalled in Google Colab) comes in.\n","\n","The good thing is, since we set up a TensorBoard callback, all of our model's training logs have been saved automatically. To visualize them, we can upload the results to [TensorBoard.dev](https://tensorboard.dev/).\n","\n","Uploading your results to TensorBoard.dev enables you to track and share multiple different modelling experiments. So if you needed to show someone your results, you could send them a link to your TensorBoard.dev as well as the accompanying Colab notebook.\n","\n","> 🔑 **Note:** These experiments are public, do not upload sensitive data. You can delete experiments if needed.\n","\n","### Uploading experiments to TensorBoard\n","\n","To upload a series of TensorFlow logs to TensorBoard, we can use the following command:\n","\n","```\n","Upload TensorBoard dev records\n","!tensorboard dev upload --logdir ./tensorflow_hub/ \\\n","  --name \"EfficientNetB0 vs. ResNet50V2\" \\ \n","  --description \"Comparing two different TF Hub feature extraction models architectures using 10% of training images\" \\ \n","  --one_shot\n","  ```\n","  \n","Where:\n","* `--logdir` is the target upload directory\n","* `--name` is the name of the experiment\n","* `--description` is a brief description of the experiment\n","* `--one_shot` exits the TensorBoard uploader once uploading is finished\n","\n","Running the `tensorboard dev upload` command will first ask you to authorize the upload to TensorBoard.dev. After you've authorized the upload, your log files will be uploaded."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-10T15:11:47.785129Z","iopub.status.busy":"2021-11-10T15:11:47.784372Z"},"trusted":true},"outputs":[],"source":["# Upload TensorBoard dev records\n","!tensorboard dev upload --logdir ./tensorflow_hub/ \\\n","  --name \"EfficientNetB0 vs. ResNet50V2\" \\\n","  --description \"Comparing two different TF Hub feature extraction models architectures using 10% of training images\" \\\n","  --one_shot"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2021-11-10T15:09:08.595665Z","iopub.status.busy":"2021-11-10T15:09:08.595291Z","iopub.status.idle":"2021-11-10T15:09:11.709788Z","shell.execute_reply":"2021-11-10T15:09:11.708995Z","shell.execute_reply.started":"2021-11-10T15:09:08.595634Z"},"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":4}
