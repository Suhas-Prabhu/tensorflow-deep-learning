{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Neural network classification with tensorflow\n\nAs the the title states, this notebook is all about training a classification model\n\nA classification model involves predicting whether something is one thing or the other.\n\nFor example, you might want to:\n- Predict whether or not someone has heart disease based on their health parameters. This is called binary classification since there are only two options.\n- Decide whether a photo of is of food, a person or a dog. This is called multi-class classification since there are more than two options.\n- Predict what categories should be assigned to a Wikipedia article. This is called multi-label classification since a single article could have more than one category assigned.\n\n## Typical architecture of a classification neural network \n\nThe word *typical* is on purpose.\n\nBecause the architecture of a classification neural network can widely vary depending on the problem you're working on.\n\nHowever, there are some fundamentals all deep neural networks contain:\n* An input layer.\n* Some hidden layers.\n* An output layer.\n\nMuch of the rest is up to the data analyst creating the model.\n\nThe following are some standard values you'll often use in your classification neural networks.\n\n| **Hyperparameter** | **Binary Classification** | **Multiclass classification** |\n| --- | --- | --- |\n| Input layer shape | Same as number of features (e.g. 5 for age, sex, height, weight, smoking status in heart disease prediction) | Same as binary classification |\n| Hidden layer(s) | Problem specific, minimum = 1, maximum = unlimited | Same as binary classification |\n| Neurons per hidden layer | Problem specific, generally 10 to 100 | Same as binary classification |\n| Output layer shape | 1 (one class or the other) | 1 per class (e.g. 3 for food, person or dog photo) |\n| Hidden activation | Usually [ReLU](https://www.kaggle.com/dansbecker/rectified-linear-units-relu-in-deep-learning) (rectified linear unit) | Same as binary classification |\n| Output activation | [Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) | [Softmax](https://en.wikipedia.org/wiki/Softmax_function) |\n| Loss function | [Cross entropy](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression) ([`tf.keras.losses.BinaryCrossentropy`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy) in TensorFlow) | Cross entropy ([`tf.keras.losses.CategoricalCrossentropy`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy) in TensorFlow) |\n| Optimizer | [SGD](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD) (stochastic gradient descent), [Adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) | Same as binary classification |\n","metadata":{}},{"cell_type":"markdown","source":"## creating data to view and fit\nIts a common practice to start off with a simple dataset before we get on the actual data.Treating it as a rehearsal\nWe can use sciki-learn's `make_circles()` function","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import make_circles\nimport tensorflow as tf\n#make 1000 examples\nn_samples = 1000\nX,y = make_circles(n_samples,noise =0.03, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:34:42.011530Z","iopub.execute_input":"2021-11-03T06:34:42.012000Z","iopub.status.idle":"2021-11-03T06:34:47.666135Z","shell.execute_reply.started":"2021-11-03T06:34:42.011892Z","shell.execute_reply":"2021-11-03T06:34:47.665096Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"X","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:34:47.668433Z","iopub.execute_input":"2021-11-03T06:34:47.668936Z","iopub.status.idle":"2021-11-03T06:34:47.680881Z","shell.execute_reply.started":"2021-11-03T06:34:47.668885Z","shell.execute_reply":"2021-11-03T06:34:47.679587Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"y","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:34:47.682709Z","iopub.execute_input":"2021-11-03T06:34:47.683582Z","iopub.status.idle":"2021-11-03T06:34:47.700716Z","shell.execute_reply.started":"2021-11-03T06:34:47.683494Z","shell.execute_reply":"2021-11-03T06:34:47.699564Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"> One of the very first steps of starting any kind of machine learning project is to *become one with the data*","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ncircles  = pd.DataFrame({\"X0\":X[:,0],\"x1\":X[:,1],\"label\":y})\ncircles.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:34:47.704257Z","iopub.execute_input":"2021-11-03T06:34:47.704713Z","iopub.status.idle":"2021-11-03T06:34:47.730303Z","shell.execute_reply.started":"2021-11-03T06:34:47.704668Z","shell.execute_reply":"2021-11-03T06:34:47.729406Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"circles.label.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:34:47.732664Z","iopub.execute_input":"2021-11-03T06:34:47.733269Z","iopub.status.idle":"2021-11-03T06:34:47.745349Z","shell.execute_reply.started":"2021-11-03T06:34:47.733224Z","shell.execute_reply":"2021-11-03T06:34:47.744079Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"We seem to be dealing with binary classification problem. If there were more label options we would be dealing with multiclass classification","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.scatter(X[:,0],X[:,1],c =y,cmap = plt.cm.RdYlBu)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:34:47.747500Z","iopub.execute_input":"2021-11-03T06:34:47.747937Z","iopub.status.idle":"2021-11-03T06:34:48.052817Z","shell.execute_reply.started":"2021-11-03T06:34:47.747892Z","shell.execute_reply":"2021-11-03T06:34:48.051816Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## input and output shapes\nlets check out the input and output shapes. This is one of the most important steps","metadata":{}},{"cell_type":"code","source":"X.shape , y.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:34:48.055371Z","iopub.execute_input":"2021-11-03T06:34:48.055792Z","iopub.status.idle":"2021-11-03T06:34:48.063679Z","shell.execute_reply.started":"2021-11-03T06:34:48.055709Z","shell.execute_reply":"2021-11-03T06:34:48.062594Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"X[0],y[0]","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:34:48.065180Z","iopub.execute_input":"2021-11-03T06:34:48.066634Z","iopub.status.idle":"2021-11-03T06:34:48.079646Z","shell.execute_reply.started":"2021-11-03T06:34:48.066589Z","shell.execute_reply":"2021-11-03T06:34:48.078401Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Alright, so we've got two X features which lead to one y value.\n\nThis means our neural network input shape will has to accept a tensor with at least one dimension being two and output a tensor with at least one value.\n\n### Steps in modelling\nNow we know what data we have as well as the input and output shapes, let's see how we'd build a neural network to model it.\n\nIn TensorFlow, there are typically 3 fundamental steps to creating and training a model.\n\n**Creating a model** - piece together the layers of a neural network yourself (using the functional or sequential API) or import a previously built model (known as transfer learning).\n\n**Compiling a model** - defining how a model's performance should be measured (loss/metrics) as well as defining how it should improve (optimizer).\n\n**Fitting a model** - letting the model try to find patterns in the data (how does X get to y).","metadata":{}},{"cell_type":"code","source":"tf.random.set_seed(42)\nmodel_1 = tf.keras.Sequential([\n    tf.keras.layers.Dense(1)\n])\n\nmodel_1.compile(loss = tf.keras.losses.BinaryCrossentropy(),\n               optimizer = tf.keras.optimizers.SGD(),\n               metrics =['accuracy'])\nmodel_1.fit(X,y, epochs =5)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:34:48.080944Z","iopub.execute_input":"2021-11-03T06:34:48.082657Z","iopub.status.idle":"2021-11-03T06:34:55.297376Z","shell.execute_reply.started":"2021-11-03T06:34:48.082610Z","shell.execute_reply":"2021-11-03T06:34:55.296301Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# since it results in poor accuracy lets train the model longer\nmodel_1.fit(X,y,epochs=199,verbose =0)\nmodel_1.evaluate(X,y)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:34:55.305585Z","iopub.execute_input":"2021-11-03T06:34:55.306104Z","iopub.status.idle":"2021-11-03T06:35:12.562847Z","shell.execute_reply.started":"2021-11-03T06:34:55.306042Z","shell.execute_reply":"2021-11-03T06:35:12.561470Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# we can see that training it longer did not help. Time to improve the model\ntf.random.set_seed(42)\nmodel_2 = tf.keras.Sequential([\n    tf.keras.layers.Dense(1),\n    tf.keras.layers.Dense(1)\n])\nmodel_2.compile(loss = tf.keras.losses.BinaryCrossentropy(),\n               optimizer = tf.keras.optimizers.SGD(),\n               metrics=[\"accuracy\"])\nmodel_2.fit(X,y,epochs =100,verbose =0)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:35:12.564459Z","iopub.execute_input":"2021-11-03T06:35:12.564940Z","iopub.status.idle":"2021-11-03T06:35:21.510324Z","shell.execute_reply.started":"2021-11-03T06:35:12.564880Z","shell.execute_reply":"2021-11-03T06:35:21.509058Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"model_2.evaluate(X,y)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:35:21.512125Z","iopub.execute_input":"2021-11-03T06:35:21.512619Z","iopub.status.idle":"2021-11-03T06:35:21.821126Z","shell.execute_reply.started":"2021-11-03T06:35:21.512573Z","shell.execute_reply":"2021-11-03T06:35:21.820087Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"still not a good accuracy. Lets use some other techniques to improve the model.","metadata":{}},{"cell_type":"code","source":"tf.random.set_seed(42)\nmodel_3 = tf.keras.Sequential()\nmodel_3.add(tf.keras.layers.Dense(100))\nmodel_3.add(tf.keras.layers.Dense(10))\nmodel_3.add(tf.keras.layers.Dense(1))\nmodel_3.compile(loss =tf.keras.losses.BinaryCrossentropy(),\n               optimizer = tf.keras.optimizers.Adam(),\n               metrics = ['accuracy'])\nmodel_3.fit(X,y,epochs=100,verbose =0)\nmodel_3.evaluate(X,y)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:35:21.824547Z","iopub.execute_input":"2021-11-03T06:35:21.824805Z","iopub.status.idle":"2021-11-03T06:35:31.873584Z","shell.execute_reply.started":"2021-11-03T06:35:21.824774Z","shell.execute_reply":"2021-11-03T06:35:31.872553Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Still there seems to be no changes made. To get to the root of the problem lets make some visualizations to see  whats happening\n> whenever a modelis performing strangely or the model isnt yielding results. Its best to always visualize, visualize the data, model and predictions\n\nTo visualize the models predictions we are going to create a plot_boundry() function that:\n   - takes in training model, X and y data as input\n   - creates a meshgrid of different x values\n   - makes predictions across the meshgrid\n   - plots predictions and a line between the zones\n   ","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndef plot_decision_bound(model,x,y):\n    # define the axis boundaries of the plot and create a meshgrid\n    x_min,x_max = x[:,0].min()-0.1, x[:,0].max()+0.1\n    y_min,y_max =x[:,1].min()-0.1, x[:,1].max()+0.1\n    xx,yy = np.meshgrid(np.linspace(x_min,x_max,100),\n                       np.linspace(y_min,y_max,100))\n    #create x values\n    x_in = np.c_[xx.ravel(),yy.ravel()]# stack 2D arrays together\n    \n    #make predictions using trained model\n    y_pred = model.predict(x_in)\n    \n    # check if its multiclass\n    if len(y_pred[0])>1:\n        print(\"proceding with multi classification\")\n        y_pred = np.argmax(y_pred,axis=1).reshape(xx.shape)\n    else:\n        print(\"doing binary classification\")\n        y_pred = np.round(y_pred).reshape(xx.shape)\n    \n    plt.contourf(xx,yy,y_pred,cmap=plt.cm.RdYlBu , alpha =0.7)\n    plt.scatter(x[:,0],x[:,1],c =y,s=40,cmap=plt.cm.RdYlBu)\n    plt.xlim(xx.min(),xx.max())\n    plt.ylim(yy.min(),yy.max())","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:35:31.875451Z","iopub.execute_input":"2021-11-03T06:35:31.875794Z","iopub.status.idle":"2021-11-03T06:35:31.888506Z","shell.execute_reply.started":"2021-11-03T06:35:31.875749Z","shell.execute_reply":"2021-11-03T06:35:31.887424Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"plot_decision_bound(model_3,X,y)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:35:31.890440Z","iopub.execute_input":"2021-11-03T06:35:31.891272Z","iopub.status.idle":"2021-11-03T06:35:32.649821Z","shell.execute_reply.started":"2021-11-03T06:35:31.891214Z","shell.execute_reply":"2021-11-03T06:35:32.648709Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"\nLooks like our model is trying to draw a straight line through the data.\n\nWhat's wrong with doing this?\n\nThe main issue is our data isn't separable by a straight line.\n\nIn a regression problem, our model might work. In fact, let's try it.","metadata":{}},{"cell_type":"code","source":"tf.random.set_seed(42)\nx_reg= np.arange(0,1000,5)\ny_reg = np.arange(100,1100,5)\n# Split it into training and test sets\nX_reg_train = x_reg[:150]\nX_reg_test = x_reg[150:]\ny_reg_train = y_reg[:150]\ny_reg_test = y_reg[150:]\n\n# Recreate the model\nmodel_3 = tf.keras.Sequential([\n  tf.keras.layers.Dense(100),\n  tf.keras.layers.Dense(10),\n  tf.keras.layers.Dense(1)\n])\n\n# Change the loss and metrics of our compiled model\nmodel_3.compile(loss=tf.keras.losses.mae, # change the loss function to be regression-specific\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=['mae']) # change the metric to be regression-specific\n\n# Fit the recompiled model\nmodel_3.fit(X_reg_train, y_reg_train, epochs=100, verbose =0)\ny_pred_reg = model_3.predict(X_reg_test)\nplt.figure(figsize=(10,7))\nplt.scatter(X_reg_train,y_reg_train,c='b',label ='training data')\nplt.scatter(X_reg_test,y_reg_test,c ='g',label =\"test data\")\nplt.scatter(X_reg_test,y_pred_reg.squeeze(),c ='r',label ='predictions')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:35:32.652939Z","iopub.execute_input":"2021-11-03T06:35:32.653335Z","iopub.status.idle":"2021-11-03T06:35:36.326663Z","shell.execute_reply.started":"2021-11-03T06:35:32.653277Z","shell.execute_reply":"2021-11-03T06:35:36.325542Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Not perfect but not bad either,model is learning but something is missing for classification problem\n\n\n#### The missing piece: Non-linearity\nOkay, so we saw our neural network can model straight lines (with ability a little bit better than guessing).\n\nWhat about non-straight (non-linear) lines?\n\nIf we're going to model our classification data (the red and clue circles), we're going to need some non-linear lines.","metadata":{}},{"cell_type":"code","source":"tf.random.set_seed(42)\nmodel_4 = tf.keras.Sequential([\n    tf.keras.layers.Dense(10,activation='relu'),\n    tf.keras.layers.Dense(5,activation = 'relu'),\n    tf.keras.layers.Dense(1,activation =\"sigmoid\")\n])\n\nmodel_4.compile(loss = tf.keras.losses.BinaryCrossentropy(),\n               optimizer = tf.keras.optimizers.Adam(),\n               metrics = [\"accuracy\"])\nhistory = model_4.fit(X,y,epochs =100, verbose =0)\nmodel_4.evaluate(X,y)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:35:36.328742Z","iopub.execute_input":"2021-11-03T06:35:36.329352Z","iopub.status.idle":"2021-11-03T06:35:47.370430Z","shell.execute_reply.started":"2021-11-03T06:35:36.329288Z","shell.execute_reply":"2021-11-03T06:35:47.369272Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"plot_decision_bound(model_4,X,y)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:35:47.372554Z","iopub.execute_input":"2021-11-03T06:35:47.373030Z","iopub.status.idle":"2021-11-03T06:35:48.041672Z","shell.execute_reply.started":"2021-11-03T06:35:47.372971Z","shell.execute_reply":"2021-11-03T06:35:48.040725Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"\nNice! It looks like our model is almost perfectly (apart from a few examples) separating the two circles.\n\n> What's wrong with the predictions we've made? Are we really evaluating our model correctly here? Hint: what data did the model learn on and what did we predict on?\n\nBefore we answer that, it's important to recognize what we've just covered.\n\n> The combination of linear (straight lines) and non-linear (non-straight lines) functions is one of the key fundamentals of neural networks.","metadata":{}},{"cell_type":"markdown","source":"lets have a look at how activation layers function on simple values","metadata":{}},{"cell_type":"code","source":"A = tf.cast(tf.range(-10,10),tf.float32)\nA","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:35:48.043245Z","iopub.execute_input":"2021-11-03T06:35:48.043773Z","iopub.status.idle":"2021-11-03T06:35:48.056489Z","shell.execute_reply.started":"2021-11-03T06:35:48.043730Z","shell.execute_reply":"2021-11-03T06:35:48.055022Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"plt.plot(A)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:35:48.058487Z","iopub.execute_input":"2021-11-03T06:35:48.059054Z","iopub.status.idle":"2021-11-03T06:35:48.324621Z","shell.execute_reply.started":"2021-11-03T06:35:48.059011Z","shell.execute_reply":"2021-11-03T06:35:48.323487Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"A straight (linear) line!\n\nNice, now let's recreate the sigmoid function and see what it does to our data. You can also find a pre-built sigmoid function at `tf.keras.activations.sigmoid`.","metadata":{}},{"cell_type":"code","source":"def sigmoid(x):\n    return 1/(1+tf.exp(-x))\n\nsigmoid(A)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:35:48.326411Z","iopub.execute_input":"2021-11-03T06:35:48.326755Z","iopub.status.idle":"2021-11-03T06:35:48.338661Z","shell.execute_reply.started":"2021-11-03T06:35:48.326710Z","shell.execute_reply":"2021-11-03T06:35:48.337565Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"plt.plot(sigmoid(A))\n# A non-straight (non-linear) line!\n# how about the ReLU function (ReLU turns all negatives to 0 and positive numbers stay the same)?","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:35:48.340335Z","iopub.execute_input":"2021-11-03T06:35:48.341130Z","iopub.status.idle":"2021-11-03T06:35:48.598386Z","shell.execute_reply.started":"2021-11-03T06:35:48.341086Z","shell.execute_reply":"2021-11-03T06:35:48.597497Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def relu(x):\n    return tf.maximum(0,x)\nrelu(A)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:35:48.600034Z","iopub.execute_input":"2021-11-03T06:35:48.600306Z","iopub.status.idle":"2021-11-03T06:35:48.611593Z","shell.execute_reply.started":"2021-11-03T06:35:48.600274Z","shell.execute_reply":"2021-11-03T06:35:48.609330Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"plt.plot(relu(A))","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:35:48.613287Z","iopub.execute_input":"2021-11-03T06:35:48.614073Z","iopub.status.idle":"2021-11-03T06:35:48.865569Z","shell.execute_reply.started":"2021-11-03T06:35:48.614025Z","shell.execute_reply":"2021-11-03T06:35:48.864496Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"\nAnother non-straight line!\n\nWell, how about TensorFlow's linear activation function?","metadata":{}},{"cell_type":"code","source":"tf.keras.activations.linear(A)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:35:48.868861Z","iopub.execute_input":"2021-11-03T06:35:48.869169Z","iopub.status.idle":"2021-11-03T06:35:48.877487Z","shell.execute_reply.started":"2021-11-03T06:35:48.869137Z","shell.execute_reply":"2021-11-03T06:35:48.876278Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"A == tf.keras.activations.linear(A)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:35:48.879411Z","iopub.execute_input":"2021-11-03T06:35:48.879822Z","iopub.status.idle":"2021-11-03T06:35:48.891536Z","shell.execute_reply.started":"2021-11-03T06:35:48.879770Z","shell.execute_reply":"2021-11-03T06:35:48.890377Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"\nOkay, so it makes sense now the model doesn't really learn anything when using only linear activation functions, because the linear activation function doesn't change our input data in anyway.\n\nWhere as, with our non-linear functions, our data gets manipulated. A neural network uses these kind of transformations at a large scale to figure draw patterns between its inputs and outputs.\n\n\n### Evaluating and improving our classification model\nIf you answered the question above, you might've picked up what we've been doing wrong.\n\nWe've been evaluating our model on the same data it was trained on.\n\nA better approach would be to split our data into training, validation (optional) and test sets.\n\nOnce we've done that, we'll train our model on the training set (let it find patterns in the data) and then see how well it learned the patterns by using it to predict values on the test set.\n\nLet's do it.","metadata":{}},{"cell_type":"code","source":"x_train,y_train = X[:800],y[:800]\nx_test,y_test = X[800:],y[800:]\nx_train.shape,y_train.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:35:48.893235Z","iopub.execute_input":"2021-11-03T06:35:48.894154Z","iopub.status.idle":"2021-11-03T06:35:48.906552Z","shell.execute_reply.started":"2021-11-03T06:35:48.894106Z","shell.execute_reply":"2021-11-03T06:35:48.905286Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"tf.random.set_seed(42)\nmodel_5 = tf.keras.Sequential([\n  tf.keras.layers.Dense(10, activation=\"relu\"), # hidden layer 1, using \"relu\" for activation (same as tf.keras.activations.relu)\n  tf.keras.layers.Dense(5, activation=\"relu\"),\n  tf.keras.layers.Dense(1, activation=\"sigmoid\") # output layer, using 'sigmoid' for the output\n])\nmodel_5.compile(loss=tf.keras.losses.binary_crossentropy,\n                optimizer=tf.keras.optimizers.Adam(lr=0.01), # increase learning rate from 0.001 to 0.01 for faster learning\n                metrics=['accuracy'])\nhistory = model_5.fit(x_train, y_train, epochs=25,verbose =0)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:35:48.913550Z","iopub.execute_input":"2021-11-03T06:35:48.913790Z","iopub.status.idle":"2021-11-03T06:35:51.982421Z","shell.execute_reply.started":"2021-11-03T06:35:48.913760Z","shell.execute_reply":"2021-11-03T06:35:51.981242Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"loss,acc = model_5.evaluate(x_test,y_test)\nprint(f\"model loss on the test set: {loss}\")\nprint(f\"model accuracy: {100*acc:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:35:51.984369Z","iopub.execute_input":"2021-11-03T06:35:51.984770Z","iopub.status.idle":"2021-11-03T06:35:52.229339Z","shell.execute_reply.started":"2021-11-03T06:35:51.984723Z","shell.execute_reply":"2021-11-03T06:35:52.228207Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# Visulize\nplt.figure(figsize =(12,6))\nplt.subplot(1,2,1)\nplt.title(\"Train\")\nplot_decision_bound(model_5,x_train,y_train)\nplt.subplot(1,2,2)\nplt.title(\"test\")\nplot_decision_bound(model_5,x_test,y_test)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:35:52.231742Z","iopub.execute_input":"2021-11-03T06:35:52.232434Z","iopub.status.idle":"2021-11-03T06:35:54.270451Z","shell.execute_reply.started":"2021-11-03T06:35:52.232383Z","shell.execute_reply":"2021-11-03T06:35:54.269468Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"### Plot the loss curves\nLooking at the plots above, we can see the outputs of our model are very good.\n\nBut how did our model go whilst it was learning?\n\nAs in, how did the performance change everytime the model had a chance to look at the data (once every epoch)?\n\nTo figure this out, we can check the loss curves (also referred to as the learning curves).\n\nYou might've seen we've been using the variable history when calling the fit() function on a model (fit() returns a History object).\n\nThis is where we'll get the information for how our model is performing as it learns.","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(history.history)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:35:54.271986Z","iopub.execute_input":"2021-11-03T06:35:54.272535Z","iopub.status.idle":"2021-11-03T06:35:54.288351Z","shell.execute_reply.started":"2021-11-03T06:35:54.272473Z","shell.execute_reply":"2021-11-03T06:35:54.287219Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(history.history).plot()\nplt.title(\"model_5 training curves\")","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:35:54.290879Z","iopub.execute_input":"2021-11-03T06:35:54.291267Z","iopub.status.idle":"2021-11-03T06:35:54.605082Z","shell.execute_reply.started":"2021-11-03T06:35:54.291219Z","shell.execute_reply":"2021-11-03T06:35:54.604105Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"\nThis is the ideal plot we'd be looking for when dealing with a classification problem, loss going down, accuracy going up.\n\n### Finding the best learning rate\nAside from the architecture itself (the layers, number of neurons, activations, etc), the most important hyperparameter you can tune for your neural network models is the learning rate.\n\nTo do so, we're going to use the following:\n\n- A learning rate callback.\n   - You can think of a callback as an extra piece of functionality you can add to your model while its training.\n- Another model (we could use the same ones as above, we we're practicing building models here).\n- A modified loss curves plot.\n","metadata":{}},{"cell_type":"code","source":"tf.random.set_seed(42)\nmodel_6 = tf.keras.Sequential([\n    tf.keras.layers.Dense(4,activation =\"relu\"),\n    tf.keras.layers.Dense(4,activation =\"relu\"),\n    tf.keras.layers.Dense(1,activation =\"sigmoid\")\n])\nmodel_6.compile(loss = tf.keras.losses.BinaryCrossentropy(),\n               optimizer = tf.keras.optimizers.Adam(),\n               metrics =['accuracy'])\n#create a scheduler callback\nlr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4*10**(epoch/20))\n# traverse a set of learning rate values starting from 1e-4, increasing by 10**(epoch/20) every epoch\nhistory = model_6.fit(x_train,y_train,epochs=100,callbacks =[lr_scheduler])","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:35:54.607723Z","iopub.execute_input":"2021-11-03T06:35:54.608768Z","iopub.status.idle":"2021-11-03T06:36:05.335102Z","shell.execute_reply.started":"2021-11-03T06:35:54.608720Z","shell.execute_reply":"2021-11-03T06:36:05.333967Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"#checkout the history\npd.DataFrame(history.history).plot(figsize=(10,7),xlabel =\"epochs\")","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:36:05.337157Z","iopub.execute_input":"2021-11-03T06:36:05.337478Z","iopub.status.idle":"2021-11-03T06:36:05.655529Z","shell.execute_reply.started":"2021-11-03T06:36:05.337404Z","shell.execute_reply":"2021-11-03T06:36:05.654472Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"\nAs you you see the learning rate exponentially increases as the number of epochs increases.\n\nAnd you can see the model's accuracy goes up (and loss goes down) at a specific point when the learning rate slowly increases.\n\nTo figure out where this infliction point is, we can plot the loss versus the log-scale learning rate","metadata":{}},{"cell_type":"code","source":"# plot the learning rate vs the loss\nlrs = 1e-4 * (10 ** (np.arange(100)/20))\nplt.figure(figsize=(10,7))\nplt.semilogx(lrs,history.history[\"loss\"]) # we want the x-axis (learning rate) to be log scale\nplt.xlabel(\"learning rate\")\nplt.ylabel(\"loss\")\nplt.title(\"Learning rate vs. loss\")","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:36:05.657080Z","iopub.execute_input":"2021-11-03T06:36:05.657579Z","iopub.status.idle":"2021-11-03T06:36:06.617608Z","shell.execute_reply.started":"2021-11-03T06:36:05.657530Z","shell.execute_reply":"2021-11-03T06:36:06.616466Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"To figure out the ideal value of the learning rate (at least the ideal value to begin training our model), the rule of thumb is to take the learning rate value where the loss is still decreasing but not quite flattened out (usually about 10x smaller than the bottom of the curve).\n\nIn this case, our ideal learning rate ends up between 0.01 ($10^{-2}$) and 0.02.","metadata":{}},{"cell_type":"code","source":"# Example of other typical learning rate values\n10**0, 10**-1, 10**-2, 10**-3, 1e-4","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:36:06.619261Z","iopub.execute_input":"2021-11-03T06:36:06.619575Z","iopub.status.idle":"2021-11-03T06:36:06.630105Z","shell.execute_reply.started":"2021-11-03T06:36:06.619533Z","shell.execute_reply":"2021-11-03T06:36:06.628921Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# model with 0.02 as the learning rate\ntf.random.set_seed(42)\n\n# Create the model\nmodel_7 = tf.keras.Sequential([\n  tf.keras.layers.Dense(4, activation=\"relu\"),\n  tf.keras.layers.Dense(4, activation=\"relu\"),\n  tf.keras.layers.Dense(1, activation=\"sigmoid\")\n])\n\n# Compile the model with the ideal learning rate\nmodel_7.compile(loss=\"binary_crossentropy\",\n                optimizer=tf.keras.optimizers.Adam(lr=0.02), # to adjust the learning rate, you need to use tf.keras.optimizers.Adam (not \"adam\")\n                metrics=[\"accuracy\"])\n\n# Fit the model for 20 epochs (5 less than before)\nhistory = model_7.fit(x_train, y_train, epochs=20)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:36:06.632090Z","iopub.execute_input":"2021-11-03T06:36:06.632490Z","iopub.status.idle":"2021-11-03T06:36:09.657710Z","shell.execute_reply.started":"2021-11-03T06:36:06.632401Z","shell.execute_reply":"2021-11-03T06:36:09.656531Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"model_7.evaluate(x_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:36:09.660012Z","iopub.execute_input":"2021-11-03T06:36:09.660651Z","iopub.status.idle":"2021-11-03T06:36:09.907562Z","shell.execute_reply.started":"2021-11-03T06:36:09.660600Z","shell.execute_reply":"2021-11-03T06:36:09.906556Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# Plot the decision boundaries for the training and test sets\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplot_decision_bound(model_7, x_train, y_train)\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplot_decision_bound(model_7, x_test, y_test)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:36:09.908995Z","iopub.execute_input":"2021-11-03T06:36:09.909323Z","iopub.status.idle":"2021-11-03T06:36:11.400543Z","shell.execute_reply.started":"2021-11-03T06:36:09.909279Z","shell.execute_reply":"2021-11-03T06:36:11.399393Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"### More classification evaluation methods\n\nAlongside the visualizations we've been making, there are a number of different evaluation metrics we can use to evaluate our classification models.\n\n| **Metric name/Evaluation method** | **Defintion** | **Code** |\n| --- | --- | --- |\n| Accuracy | Out of 100 predictions, how many does your model get correct? E.g. 95% accuracy means it gets 95/100 predictions correct. | [`sklearn.metrics.accuracy_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) or [`tf.keras.metrics.Accuracy()`](tensorflow.org/api_docs/python/tf/keras/metrics/Accuracy) |\n| Precision | Proportion of true positives over total number of samples. Higher precision leads to less false positives (model predicts 1 when it should've been 0). | [`sklearn.metrics.precision_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html) or [`tf.keras.metrics.Precision()`](tensorflow.org/api_docs/python/tf/keras/metrics/Precision) |\n| Recall | Proportion of true positives over total number of true positives and false negatives (model predicts 0 when it should've been 1). Higher recall leads to less false negatives. | [`sklearn.metrics.recall_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html) or [`tf.keras.metrics.Recall()`](tensorflow.org/api_docs/python/tf/keras/metrics/Recall) |\n| F1-score | Combines precision and recall into one metric. 1 is best, 0 is worst. | [`sklearn.metrics.f1_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) |\n| [Confusion matrix](https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/)  | Compares the predicted values with the true values in a tabular way, if 100% correct, all values in the matrix will be top left to bottom right (diagnol line). | Custom function or [`sklearn.metrics.plot_confusion_matrix()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html) |\n| Classification report | Collection of some of the main classification metrics such as precision, recall and f1-score. | [`sklearn.metrics.classification_report()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) |\n\n","metadata":{}},{"cell_type":"code","source":"loss,acc = model_7.evaluate(x_test,y_test)\nprint(f\"loss on test: {loss}\")\nprint(f\"accuracy on test: {acc}\")","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:36:11.404463Z","iopub.execute_input":"2021-11-03T06:36:11.404744Z","iopub.status.idle":"2021-11-03T06:36:11.496081Z","shell.execute_reply.started":"2021-11-03T06:36:11.404713Z","shell.execute_reply":"2021-11-03T06:36:11.494795Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"We can make a confusion matrix using Scikit-Learn's confusion_matrix method.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\ny_preds = model_7.predict(x_test)\nconfusion_matrix(y_test,y_preds)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:36:11.499769Z","iopub.execute_input":"2021-11-03T06:36:11.500016Z","iopub.status.idle":"2021-11-03T06:36:11.962107Z","shell.execute_reply.started":"2021-11-03T06:36:11.499984Z","shell.execute_reply":"2021-11-03T06:36:11.960547Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"y_test[:10],y_preds[:10]","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:53:07.992977Z","iopub.execute_input":"2021-11-03T06:53:07.993895Z","iopub.status.idle":"2021-11-03T06:53:08.010725Z","shell.execute_reply.started":"2021-11-03T06:53:07.993851Z","shell.execute_reply":"2021-11-03T06:53:08.009806Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"It looks like we need to get our predictions into the binary format (0 or 1).\n\nBut you might be wondering, what format are they currently in?\n\nIn their current format (9.8526537e-01), they're in a form called **prediction probabilities.**\n\nYou'll see this often with the outputs of neural networks. Often they won't be exact values but more a probability of how likely they are to be one value or another.\n\nSo one of the steps you'll often see after making predicitons with a neural network is converting the prediction probabilities into labels.\n\nIn our case, since our ground truth labels (y_test) are binary (0 or 1), we can convert the prediction probabilities using to their binary form using `tf.round().`","metadata":{}},{"cell_type":"code","source":"tf.round(y_preds)[:10]","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:53:08.872309Z","iopub.execute_input":"2021-11-03T06:53:08.872620Z","iopub.status.idle":"2021-11-03T06:53:08.889970Z","shell.execute_reply.started":"2021-11-03T06:53:08.872587Z","shell.execute_reply":"2021-11-03T06:53:08.888086Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(y_test,tf.round(y_preds))","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:53:08.892485Z","iopub.execute_input":"2021-11-03T06:53:08.892860Z","iopub.status.idle":"2021-11-03T06:53:08.905815Z","shell.execute_reply.started":"2021-11-03T06:53:08.892816Z","shell.execute_reply":"2021-11-03T06:53:08.904572Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"# lets visualize the same\nfigs =(10,10)\ncm  = confusion_matrix(y_test,tf.round(y_preds))\ncm_norm = cm.astype(\"float\")/cm.sum(axis =1)[:,np.newaxis] # normalize it\nn_classes = cm.shape[0]\n\nfig,ax = plt.subplots(figsize =figs)\n# matrix plot\ncax = ax.matshow(cm, cmap =plt.cm.Blues)\nfig.colorbar(cax)\n# create classes\nclasses = False\nif classes:\n    labels = classes\nelse:\n    labels = np.arange(cm.shape[0])\n    \nax.set(title =\"confusion matrix\",\n      xlabel =\"predicted label\",\n      ylabel =\"true label\",\n      xticks = np.arange(n_classes),\n      yticks = np.arange(n_classes),\n      xticklabels = labels,\n      yticklabels = labels)\n\n# set x-axis  labels to bottom\nax.xaxis.set_label_position(\"bottom\")\nax.xaxis.tick_bottom()\n\n# adjust label size\nax.xaxis.label.set_size(20)\nax.yaxis.label.set_size(20)\nax.title.set_size(20)\n\nthreshold = (cm.max()+cm.min())/2\nimport itertools\nfor i, j in itertools.product(range(cm.shape[0]),range(cm.shape[1])):\n    plt.text(j,i, f\"{cm[i,j]} ({cm_norm[i,j]*100:.1f}%)\",\n            horizontalalignment=\"center\",\n            color =\"white\" if cm[i,j]> threshold else \"black\", size =15)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:53:08.907730Z","iopub.execute_input":"2021-11-03T06:53:08.908538Z","iopub.status.idle":"2021-11-03T06:53:09.259559Z","shell.execute_reply.started":"2021-11-03T06:53:08.908466Z","shell.execute_reply":"2021-11-03T06:53:09.258530Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"### Working with a larger example (multiclass classification)\n\n\nFor example, say you were a fashion company and you wanted to build a neural network to predict whether a piece of clothing was a shoe, a shirt or a jacket (3 different options).\n\nWhen you have more than two classes as an option, this is known as multiclass classification.\n\nThe good news is, the things we've learned so far (with a few tweaks) can be applied to multiclass classification problems as well.\n\nLet's see it in action.\n\nTo start, we'll need some data. The good thing for us is TensorFlow has a multiclass classication dataset known as Fashion MNIST built-in. Meaning we can get started straight away.\n\nWe can import it using the` tf.keras.datasets module.`\n","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.datasets import fashion_mnist\n\n# The data has already been sorted into training and test sets \n(train_data,train_labels),(test_data,test_labels) = fashion_mnist.load_data()","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:53:09.262135Z","iopub.execute_input":"2021-11-03T06:53:09.262711Z","iopub.status.idle":"2021-11-03T06:53:11.264557Z","shell.execute_reply.started":"2021-11-03T06:53:09.262662Z","shell.execute_reply":"2021-11-03T06:53:11.263407Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"# Check the shapes of the dataset\ntrain_data.shape, train_labels.shape, test_data.shape, test_labels.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:53:11.267046Z","iopub.execute_input":"2021-11-03T06:53:11.267362Z","iopub.status.idle":"2021-11-03T06:53:11.274977Z","shell.execute_reply.started":"2021-11-03T06:53:11.267332Z","shell.execute_reply":"2021-11-03T06:53:11.273915Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"So we have 60000 training data and 10000 test data, with each data having shape of (28, 28) and a label.","metadata":{}},{"cell_type":"code","source":"#lets visualize\nimport matplotlib.pyplot as plt\nplt.imshow(train_data[7])","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:53:11.276998Z","iopub.execute_input":"2021-11-03T06:53:11.277749Z","iopub.status.idle":"2021-11-03T06:53:11.529466Z","shell.execute_reply.started":"2021-11-03T06:53:11.277703Z","shell.execute_reply":"2021-11-03T06:53:11.528390Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":" train_labels[7]","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:53:11.532432Z","iopub.execute_input":"2021-11-03T06:53:11.533093Z","iopub.status.idle":"2021-11-03T06:53:11.542922Z","shell.execute_reply.started":"2021-11-03T06:53:11.533046Z","shell.execute_reply":"2021-11-03T06:53:11.541812Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"It looks like our labels are in numerical form. And while this is fine for a neural network, you might want to have them in human readable form.\n\nLet's create a small list of the class names \n","metadata":{}},{"cell_type":"code","source":"class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\nlen(class_names)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:53:11.545765Z","iopub.execute_input":"2021-11-03T06:53:11.546426Z","iopub.status.idle":"2021-11-03T06:53:11.556180Z","shell.execute_reply.started":"2021-11-03T06:53:11.546382Z","shell.execute_reply":"2021-11-03T06:53:11.554911Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"# Plot an example image and its label\nplt.imshow(train_data[17], cmap=plt.cm.binary) # change the colours to black & white\nplt.title(class_names[train_labels[17]]);","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:53:11.558124Z","iopub.execute_input":"2021-11-03T06:53:11.558990Z","iopub.status.idle":"2021-11-03T06:53:11.811592Z","shell.execute_reply.started":"2021-11-03T06:53:11.558930Z","shell.execute_reply":"2021-11-03T06:53:11.810489Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"# Plot multiple random images of fashion MNIST\nimport random\nplt.figure(figsize=(7, 7))\nfor i in range(4):\n  ax = plt.subplot(2, 2, i + 1)\n  rand_index = random.choice(range(len(train_data)))\n  plt.imshow(train_data[rand_index], cmap=plt.cm.binary)\n  plt.title(class_names[train_labels[rand_index]])\n  plt.axis(False)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:53:11.813372Z","iopub.execute_input":"2021-11-03T06:53:11.813722Z","iopub.status.idle":"2021-11-03T06:53:12.178986Z","shell.execute_reply.started":"2021-11-03T06:53:11.813682Z","shell.execute_reply":"2021-11-03T06:53:12.178052Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"\nAlright, let's build a model to figure out the relationship between the pixel values and their labels.\n\nSince this is a multiclass classification problem, we'll need to make a few changes to our architecture (inline with Table 1 above):\n\n- The input shape will have to deal with 28x28 tensors (the height and width of our images).\n    - We're actually going to squash the input into a tensor (vector) of shape (784).\n- The output shape will have to be 10 because we need our model to predict for 10 different classes.\n     - We'll also change the activation parameter of our output layer to be \"softmax\" instead of 'sigmoid'. As we'll see the \"softmax\" activation function outputs a series of values between 0 & 1 (the same shape as output shape, which together add up to ~1. The index with the highest value is predicted by the model to be the most likely class.\n- We'll need to change our loss function from a binary loss function to a multiclass loss function.\nMore specifically, since our labels are in integer form, we'll use tf.keras.losses.SparseCategoricalCrossentropy(), if our labels were one-hot encoded (e.g. they looked something like [0, 0, 1, 0, 0...]), we'd use tf.keras.losses.CategoricalCrossentropy().\n- We'll also use the validation_data parameter when calling the fit() function. This will give us an idea of how the model performs on the test set during training.","metadata":{}},{"cell_type":"code","source":"tf.random.set_seed(42)\nmodel_8 = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape =(28,28)), # flattening the input layer from 28X28 to 784\n    tf.keras.layers.Dense(4,activation =\"relu\"),\n    tf.keras.layers.Dense(4,activation =\"relu\"),\n    tf.keras.layers.Dense(10,activation =\"softmax\"),\n])\nmodel_8.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n               optimizer = tf.keras.optimizers.Adam(),\n               metrics=['accuracy'])\nhistory = model_8.fit(train_data,train_labels,epochs =10,validation_data =(test_data, test_labels))","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:53:24.443968Z","iopub.execute_input":"2021-11-03T06:53:24.444334Z","iopub.status.idle":"2021-11-03T06:54:17.920960Z","shell.execute_reply.started":"2021-11-03T06:53:24.444302Z","shell.execute_reply":"2021-11-03T06:54:17.919995Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"model_8.summary()","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:54:17.923730Z","iopub.execute_input":"2021-11-03T06:54:17.924104Z","iopub.status.idle":"2021-11-03T06:54:17.935573Z","shell.execute_reply.started":"2021-11-03T06:54:17.924058Z","shell.execute_reply":"2021-11-03T06:54:17.934337Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"\nAlright, our model gets to about ~35% accuracy after 10 epochs using a similar style model to what we used on our binary classification problem.\n\nWhich is better than guessing (guessing with 10 classes would result in about 10% accuracy) but we can do better.\n\nDo you remember when we talked about neural networks preferring numbers between 0 and 1? (if not, treat this as a reminder)\n\nWell, right now, the data we have isn't between 0 and 1, in other words, it's not normalized (hence why we used the non_norm_history variable when calling fit()). It's pixel values are between 0 and 255.\n\nLet's see.","metadata":{}},{"cell_type":"code","source":"#lets normalize the data\ntrain_data = train_data/255.0\ntest_data = test_data/255.0\n\ntrain_data.min(),train_data.max()","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:54:17.936994Z","iopub.execute_input":"2021-11-03T06:54:17.938036Z","iopub.status.idle":"2021-11-03T06:54:18.247455Z","shell.execute_reply.started":"2021-11-03T06:54:17.937998Z","shell.execute_reply":"2021-11-03T06:54:18.246484Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"# Set random seed\ntf.random.set_seed(42)\n\n# Create the model\nmodel_9 = tf.keras.Sequential([\n  tf.keras.layers.Flatten(input_shape=(28, 28)), # input layer (we had to reshape 28x28 to 784)\n  tf.keras.layers.Dense(4, activation=\"relu\"),\n  tf.keras.layers.Dense(4, activation=\"relu\"),\n  tf.keras.layers.Dense(10, activation=\"softmax\") # output shape is 10, activation is softmax\n])\n\n# Compile the model\nmodel_9.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n                 optimizer=tf.keras.optimizers.Adam(),\n                 metrics=[\"accuracy\"])\n\n# Create the learning rate callback\nlr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3 * 10**(epoch/20))\n\n# Fit the model\nfind_lr_history = model_9.fit(train_data,\n                               train_labels,\n                               epochs=40, # model already doing pretty good with current LR, probably don't need 100 epochs\n                               validation_data=(test_data, test_labels),\n                               callbacks=[lr_scheduler])","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:54:18.250274Z","iopub.execute_input":"2021-11-03T06:54:18.251706Z","iopub.status.idle":"2021-11-03T06:58:41.055009Z","shell.execute_reply.started":"2021-11-03T06:54:18.251654Z","shell.execute_reply":"2021-11-03T06:58:41.053993Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"lrs = 1e-3*(10**(np.arange(40)/20))\nplt.semilogx(lrs, find_lr_history.history[\"loss\"]) # want the x-axis to be log-scale\nplt.xlabel(\"Learning rate\")\nplt.ylabel(\"Loss\")\nplt.title(\"Finding the ideal learning rate\");","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:58:41.056936Z","iopub.execute_input":"2021-11-03T06:58:41.057238Z","iopub.status.idle":"2021-11-03T06:58:41.616377Z","shell.execute_reply.started":"2021-11-03T06:58:41.057194Z","shell.execute_reply":"2021-11-03T06:58:41.615318Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"# lets make some changes , The optimum learning rate seems too be 0.001\ntf.random.set_seed(42)\nmodel_10 = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=(28,28)),\n    tf.keras.layers.Dense(100,activation = \"relu\"),\n    tf.keras.layers.Dense(50,activation= \"relu\"),\n    tf.keras.layers.Dense(10,activation =\"softmax\")\n])\nmodel_10.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n                optimizer = tf.keras.optimizers.Adam(lr =0.001),\n                metrics  =[\"accuracy\"] \n                )\nhistory=model_10.fit(train_data,train_labels,epochs = 50,validation_data=(test_data,test_labels))","metadata":{"execution":{"iopub.status.busy":"2021-11-03T06:58:41.618133Z","iopub.execute_input":"2021-11-03T06:58:41.618679Z","iopub.status.idle":"2021-11-03T07:04:04.458023Z","shell.execute_reply.started":"2021-11-03T06:58:41.618626Z","shell.execute_reply":"2021-11-03T07:04:04.456722Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(history.history).plot()","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:04:04.459984Z","iopub.execute_input":"2021-11-03T07:04:04.460368Z","iopub.status.idle":"2021-11-03T07:04:04.881612Z","shell.execute_reply.started":"2021-11-03T07:04:04.460307Z","shell.execute_reply":"2021-11-03T07:04:04.880703Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"import itertools\nfrom sklearn.metrics import confusion_matrix\n\ndef make_confusion_matrix(y_true,y_pred,classes =None,figsize =(10,10),text_size =15):\n    \"\"\"Makes a labelled confusion matrix comparing predictions and ground truth labels.\n\n  If classes is passed, confusion matrix will be labelled, if not, integer class values\n  will be used.\n\n  Args:\n    y_true: Array of truth labels (must be same shape as y_pred).\n    y_pred: Array of predicted labels (must be same shape as y_true).\n    classes: Array of class labels (e.g. string form). If `None`, integer labels are used.\n    figsize: Size of output figure (default=(10, 10)).\n    text_size: Size of output figure text (default=15).\n  \n  Returns:\n    A labelled confusion matrix plot comparing y_true and y_pred.\n\n  Example usage:\n    make_confusion_matrix(y_true=test_labels, # ground truth test labels\n                          y_pred=y_preds, # predicted labels\n                          classes=class_names, # array of class label names\n                          figsize=(15, 15),\n                          text_size=10)\n    \"\"\"\n    cm = confusion_matrix(y_true,y_pred)\n    cm_norm = cm.astype(\"float\")/cm.sum(axis=1)[:,np.newaxis]\n    n_classes = cm.shape[0]\n    fig,ax = plt.subplots(figsize = figsize)\n    cax = ax.matshow(cm,cmap =plt.cm.Blues)# colors will represent how 'correct' a class is, darker == better\n    fig.colorbar(cax)\n    if classes:\n        labels = classes\n    else:\n        labels = np.arange(cm.shape[0])        \n    ax.set(title =\"confusion matrix\",\n              xlabel = \"predicted label\",\n              ylabel = \"True label\",\n              xticks = np.arange(n_classes),\n              yticks = np.arange(n_classes),\n              xticklabels = labels,\n              yticklabels = labels,)\n    ax.xaxis.set_label_position(\"bottom\")\n    ax.xaxis.tick_bottom()\n    threshold = (cm.max()+cm.min())/2.0\n    #plot the text on each cell\n    for i, j in itertools.product(range(cm.shape[0]),range(cm.shape[1])):\n        plt.text(j,i,f\"{cm[i,j]}({cm_norm[i,j]*100:.1f}%)\", horizontalalignment =\"center\",color =\"white\" if cm[i,j] > threshold else \"black\",\n                    size = text_size)\n        ","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:20:24.486849Z","iopub.execute_input":"2021-11-03T07:20:24.487204Z","iopub.status.idle":"2021-11-03T07:20:24.500743Z","shell.execute_reply.started":"2021-11-03T07:20:24.487156Z","shell.execute_reply":"2021-11-03T07:20:24.499492Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"y_probs = model_10.predict(test_data)\ny_probs[:5]","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:19:14.345774Z","iopub.execute_input":"2021-11-03T07:19:14.346078Z","iopub.status.idle":"2021-11-03T07:19:15.085587Z","shell.execute_reply.started":"2021-11-03T07:19:14.346046Z","shell.execute_reply":"2021-11-03T07:19:15.084522Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"markdown","source":"Our model outputs a list of prediction probabilities, meaning, it outputs a number for how likely it thinks a particular class is to be the label.\n\nThe higher the number in the prediction probabilities list, the more likely the model believes that is the right class.\n\nTo find the highest value we can use the argmax() method.","metadata":{}},{"cell_type":"code","source":"y_probs[0].argmax()","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:19:18.400113Z","iopub.execute_input":"2021-11-03T07:19:18.400417Z","iopub.status.idle":"2021-11-03T07:19:18.407839Z","shell.execute_reply.started":"2021-11-03T07:19:18.400385Z","shell.execute_reply":"2021-11-03T07:19:18.406803Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"y_pred = y_probs.argmax(axis =1)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:19:21.695992Z","iopub.execute_input":"2021-11-03T07:19:21.696639Z","iopub.status.idle":"2021-11-03T07:19:21.703598Z","shell.execute_reply.started":"2021-11-03T07:19:21.696604Z","shell.execute_reply":"2021-11-03T07:19:21.702190Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(y_true = test_labels,y_pred =y_pred)\ncm","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:19:23.741473Z","iopub.execute_input":"2021-11-03T07:19:23.741866Z","iopub.status.idle":"2021-11-03T07:19:23.797119Z","shell.execute_reply.started":"2021-11-03T07:19:23.741833Z","shell.execute_reply":"2021-11-03T07:19:23.795772Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"#lets visualize the above\nmake_confusion_matrix(test_labels,y_pred,class_names,text_size =8)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:21:28.005796Z","iopub.execute_input":"2021-11-03T07:21:28.006131Z","iopub.status.idle":"2021-11-03T07:21:29.833098Z","shell.execute_reply.started":"2021-11-03T07:21:28.006070Z","shell.execute_reply":"2021-11-03T07:21:29.831787Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"import random\ndef plot_rand_img(model,images,true_labels,classes):\n    \"\"\"Picks a random image, plots it and labels it with a predicted and truth label.\n\n  Args:\n    model: a trained model (trained on data similar to what's in images).\n    images: a set of random images (in tensor form).\n    true_labels: array of ground truth labels for images.\n    classes: array of class names for images.\n  \n  Returns:\n    A plot of a random image from `images` with a predicted class label from `model`\n    as well as the truth class label from `true_labels`.\n    \"\"\"\n    i = random.randint(0,len(images))\n    target_image = images[i]\n    pred_prob = model.predict(target_image.reshape(1,28,28))\n    pred_label = classes[pred_prob.argmax()]\n    true_label = classes[true_labels[i]]\n    \n    #plot the target image\n    plt.imshow(target_image,cmap = plt.cm.binary)\n    \n    if pred_label == true_label:\n        color = \"green\"\n    else:\n        color = \"red\"\n    plt.xlabel(\"pred: {} {:2.0f}%(True:{})\".format(pred_label, 100*tf.reduce_max(pred_prob),true_label),color = color)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:29:37.078873Z","iopub.execute_input":"2021-11-03T07:29:37.079232Z","iopub.status.idle":"2021-11-03T07:29:37.087837Z","shell.execute_reply.started":"2021-11-03T07:29:37.079200Z","shell.execute_reply":"2021-11-03T07:29:37.086599Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"# Check out a random image as well as its prediction\nplot_rand_img(model=model_10, \n                  images=test_data, \n                  true_labels=test_labels, \n                  classes=class_names)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:31:06.925437Z","iopub.execute_input":"2021-11-03T07:31:06.925959Z","iopub.status.idle":"2021-11-03T07:31:07.312675Z","shell.execute_reply.started":"2021-11-03T07:31:06.925915Z","shell.execute_reply":"2021-11-03T07:31:07.311570Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"# Find the layers of our most recent model\nmodel_10.layers","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:32:15.478050Z","iopub.execute_input":"2021-11-03T07:32:15.478371Z","iopub.status.idle":"2021-11-03T07:32:15.489981Z","shell.execute_reply.started":"2021-11-03T07:32:15.478338Z","shell.execute_reply":"2021-11-03T07:32:15.488892Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"code","source":"#we can access the layers \nmodel_10.layers[1]","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:32:56.663893Z","iopub.execute_input":"2021-11-03T07:32:56.664199Z","iopub.status.idle":"2021-11-03T07:32:56.671521Z","shell.execute_reply.started":"2021-11-03T07:32:56.664169Z","shell.execute_reply":"2021-11-03T07:32:56.670185Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"# we can find the pattern learnt at a particular layer\nweights , biases = model_10.layers[1].get_weights()\nweights, weights.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:35:25.661293Z","iopub.execute_input":"2021-11-03T07:35:25.661733Z","iopub.status.idle":"2021-11-03T07:35:25.685298Z","shell.execute_reply.started":"2021-11-03T07:35:25.661676Z","shell.execute_reply":"2021-11-03T07:35:25.684389Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"markdown","source":"The weights matrix is the same shape as the input data, which in our case is 784 (28x28 pixels). And there's a copy of the weights matrix for each neuron the in the selected layer (our selected layer has 4 neurons).\n\nEach value in the weights matrix corresponds to how a particular value in the input data influences the network's decisions.\n\nThese values start out as random numbers (they're set by the kernel_initializer parameter when creating a layer, the default is \"glorot_uniform\") and are then updated to better representative values of the data (non-random) by the neural network during training.","metadata":{}},{"cell_type":"code","source":"biases, biases.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:37:17.751367Z","iopub.execute_input":"2021-11-03T07:37:17.751664Z","iopub.status.idle":"2021-11-03T07:37:17.760142Z","shell.execute_reply.started":"2021-11-03T07:37:17.751632Z","shell.execute_reply":"2021-11-03T07:37:17.759132Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"markdown","source":"\nEvery neuron has a bias vector. Each of these is paired with a weight matrix.\n\nThe bias values get initialized as zeroes by default (using the bias_initializer parameter).\n\n The bias vector dictates how much the patterns within the corresponding weights matrix should influence the next layer.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.utils import plot_model\nplot_model(model_10,show_shapes =True)","metadata":{"execution":{"iopub.status.busy":"2021-11-03T07:39:27.741074Z","iopub.execute_input":"2021-11-03T07:39:27.741342Z","iopub.status.idle":"2021-11-03T07:39:28.697624Z","shell.execute_reply.started":"2021-11-03T07:39:27.741304Z","shell.execute_reply":"2021-11-03T07:39:28.696349Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"markdown","source":"\nHow a model learns (in brief)\nAlright, we've trained a bunch of models, but we've never really discussed what's going on under the hood. So how exactly does a model learn?\n\nA model learns by updating and improving its weight matrices and biases values every epoch (in our case, when we call the fit() fucntion).\n\nIt does so by comparing the patterns its learned between the data and labels to the actual labels.\n\nIf the current patterns (weight matrices and bias values) don't result in a desirable decrease in the loss function (higher loss means worse predictions), the optimizer tries to steer the model to update its patterns in the right way (using the real labels as a reference).\n\nThis process of using the real labels as a reference to improve the model's predictions is called backpropagation.\n\nIn other words, data and labels pass through a model (forward pass) and it attempts to learn the relationship between the data and labels.\n\nAnd if this learned relationship isn't close to the actual relationship or it could be improved, the model does so by going back through itself (backward pass) and tweaking its weights matrices and bias values to better represent the data.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}