{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Convolution neural network with Tensorflow\n\nLets start off by getting the data. As convolution neural networks work well with images, we can start off with a dataset of images.\n\nWe are gonna use two categories from the FOOD-101 dataset and build a binary classifier.","metadata":{}},{"cell_type":"code","source":"import zipfile\n\n!wget https://storage.googleapis.com/ztm_tf_course/food_vision/pizza_steak.zip\n\nzip_ref = zipfile.ZipFile(\"pizza_steak.zip\",\"r\")\nzip_ref.extractall()\nzip_ref.close()","metadata":{"execution":{"iopub.status.busy":"2021-11-09T13:48:18.588903Z","iopub.execute_input":"2021-11-09T13:48:18.589225Z","iopub.status.idle":"2021-11-09T13:48:22.260017Z","shell.execute_reply.started":"2021-11-09T13:48:18.589129Z","shell.execute_reply":"2021-11-09T13:48:22.259143Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Inspect the data and be one with it\n\nThe file structure has been formated to directories and subdirectories to use it for cnns\n\nMore specifically:\n\n- A train directory which contains all of the images in the training dataset with subdirectories each named after a certain class containing images of that class.\n- A test directory with the same structure as the train directory.","metadata":{}},{"cell_type":"code","source":"import os \nnum_steak = len(os.listdir(\"../working/pizza_steak/train/steak/\"))\nnum_steak","metadata":{"execution":{"iopub.status.busy":"2021-11-09T13:48:22.263733Z","iopub.execute_input":"2021-11-09T13:48:22.264388Z","iopub.status.idle":"2021-11-09T13:48:22.275757Z","shell.execute_reply.started":"2021-11-09T13:48:22.264336Z","shell.execute_reply":"2021-11-09T13:48:22.275045Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# getting the class names programmatically \nimport pathlib\nimport numpy as np\ndata_dir = pathlib.Path(\"../working/pizza_steak/train\")\n# create a list of class names from the sub directories\nclass_names = np.array(sorted([item.name for item in data_dir.glob('*')]))\nprint(class_names)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T13:48:22.277382Z","iopub.execute_input":"2021-11-09T13:48:22.277952Z","iopub.status.idle":"2021-11-09T13:48:22.285375Z","shell.execute_reply.started":"2021-11-09T13:48:22.277895Z","shell.execute_reply":"2021-11-09T13:48:22.284414Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### visualize visualize visualize visualize","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.image as npimg\nimport random\n\ndef view_random_image(target_dir,target_class):\n    # setup target directory\n    target_folder = target_dir+target_class\n    \n    rand_img = random.sample(os.listdir(target_folder),1)\n    \n    img = npimg.imread(target_folder +'/'+rand_img[0])\n    plt.imshow(img)\n    plt.title(target_class)\n    plt.axis(\"off\")\n    \n    print(f\"Image shape: {img.shape}\")\n    return img","metadata":{"execution":{"iopub.status.busy":"2021-11-09T13:48:22.287688Z","iopub.execute_input":"2021-11-09T13:48:22.288557Z","iopub.status.idle":"2021-11-09T13:48:22.295671Z","shell.execute_reply.started":"2021-11-09T13:48:22.288497Z","shell.execute_reply":"2021-11-09T13:48:22.294785Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"img = view_random_image(target_dir =\"../working/pizza_steak/train/\",target_class = \"steak\")","metadata":{"execution":{"iopub.status.busy":"2021-11-09T13:48:22.297199Z","iopub.execute_input":"2021-11-09T13:48:22.297520Z","iopub.status.idle":"2021-11-09T13:48:22.517772Z","shell.execute_reply.started":"2021-11-09T13:48:22.297465Z","shell.execute_reply":"2021-11-09T13:48:22.517014Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Looking at the image shape more closely, you'll see it's in the form (Width, Height, Colour Channels).\n\nIn our case, the widthand height may vary but all are colour images. The values in the R,G and B channels will vary from 0 to 255\n\nSo in convolution neural networks, the network looks for patterns in those values in the 3 channels\n\n>As we've discussed before, many machine learning models, including neural networks prefer the values they work with to be between 0 and 1. Knowing this, one of the most common preprocessing steps for working with images is to scale (also referred to as normalize) their pixel values by dividing the image arrays by 255.","metadata":{}},{"cell_type":"code","source":"img/255.","metadata":{"execution":{"iopub.status.busy":"2021-11-09T13:48:22.519026Z","iopub.execute_input":"2021-11-09T13:48:22.519328Z","iopub.status.idle":"2021-11-09T13:48:22.531269Z","shell.execute_reply.started":"2021-11-09T13:48:22.519293Z","shell.execute_reply":"2021-11-09T13:48:22.530154Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Components of a convolutional neural network:\n\n| **Hyperparameter/Layer type** | **What does it do?** | **Typical values** |\n| ----- | ----- | ----- |\n| Input image(s) | Target images you'd like to discover patterns in| Whatever you can take a photo (or video) of |\n| Input layer | Takes in target images and preprocesses them for further layers | `input_shape = [batch_size, image_height, image_width, color_channels]` |\n| Convolution layer | Extracts/learns the most important features from target images | Multiple, can create with [`tf.keras.layers.ConvXD`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) (X can be multiple values) |\n| Hidden activation | Adds non-linearity to learned features (non-straight lines) | Usually ReLU ([`tf.keras.activations.relu`](https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu)) |\n| Pooling layer | Reduces the dimensionality of learned image features | Average ([`tf.keras.layers.AvgPool2D`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/AveragePooling2D)) or Max ([`tf.keras.layers.MaxPool2D`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D)) |\n| Fully connected layer | Further refines learned features from convolution layers | [`tf.keras.layers.Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) |\n| Output layer | Takes learned features and outputs them in shape of target labels | `output_shape = [number_of_classes]` (e.g. 3 for pizza, steak or sushi)|\n| Output activation | Adds non-linearities to output layer | [`tf.keras.activations.sigmoid`](https://www.tensorflow.org/api_docs/python/tf/keras/activations/sigmoid) (binary classification) or [`tf.keras.activations.softmax`](https://www.tensorflow.org/api_docs/python/tf/keras/activations/softmax) |\n\nHow they stack together:\n\n![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-simple-convnet.png)\n*A simple example of how you might stack together the above layers into a convolutional neural network. Note the convolutional and pooling layers can often be arranged and rearranged into many different formations.*\n\nFor reference, the model we're using replicates TinyVGG, the computer vision architecture which fuels the CNN explainer webpage.\n\n> The architecture we're using below is a scaled-down version of VGG-16, a convolutional neural network which came 2nd in the 2014 ImageNet classification competition.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n#  set the seed\ntf.random.set_seed(42)\n\n# normalizing the data\ntrain_datagen = ImageDataGenerator(rescale = 1./255)\nvalid_datagen = ImageDataGenerator(rescale =1./255)\n\ntrain_dir = \"../working/pizza_steak/train/\"\ntest_dir = \"../working/pizza_steak/test/\"\n\ntrain_data = train_datagen.flow_from_directory(train_dir,\n                                              batch_size = 32, # number of images to be processed at the time\n                                              target_size = (224,224), # convert all images to the size of 224X224\n                                              class_mode =\"binary\", # type of problem we're working on\n                                              seed = 42)\nvalid_data = valid_datagen.flow_from_directory(test_dir,\n                                              batch_size = 32,\n                                              target_size = (224,224),\n                                              class_mode =\"binary\",\n                                              seed = 42)\n# Create a CNN model\nmodel_1  = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(filters =10,\n                          kernel_size = (3,3),\n                          activation =\"relu\",\n                          input_shape = (224,224,3)), # first layer specifies input shape\n    tf.keras.layers.Conv2D(10,(3,3),activation='relu'),\n    tf.keras.layers.MaxPool2D(pool_size=(2,2), padding =\"valid\"),\n    tf.keras.layers.Conv2D(10,(3,3),activation =\"relu\"),\n    tf.keras.layers.Conv2D(10,(3,3),activation =\"relu\"),\n    tf.keras.layers.MaxPool2D((2,2)),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(1,activation =\"sigmoid\") # binary actiavtion output\n])\n\n# compile the model\n\nmodel_1.compile(loss = \"binary_crossentropy\",\n               optimizer = tf.keras.optimizers.Adam(),\n               metrics =[\"accuracy\"])\n\nhistory_1 = model_1.fit(train_data,\n                       epochs =5,\n                       steps_per_epoch = len(train_data),\n                       validation_data = valid_data,\n                       validation_steps = len(valid_data))","metadata":{"execution":{"iopub.status.busy":"2021-11-09T13:48:22.533105Z","iopub.execute_input":"2021-11-09T13:48:22.533613Z","iopub.status.idle":"2021-11-09T13:49:25.813694Z","shell.execute_reply.started":"2021-11-09T13:48:22.533572Z","shell.execute_reply":"2021-11-09T13:49:25.812781Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"We can see that the model gives a very good accuracy within 5 epochs, but we are training only on 2 of the classes rather than 101 classes\n","metadata":{}},{"cell_type":"code","source":"model_1.summary()","metadata":{"execution":{"iopub.status.busy":"2021-11-09T13:49:25.815573Z","iopub.execute_input":"2021-11-09T13:49:25.815869Z","iopub.status.idle":"2021-11-09T13:49:25.828177Z","shell.execute_reply.started":"2021-11-09T13:49:25.815828Z","shell.execute_reply":"2021-11-09T13:49:25.827262Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"\n**Binary classification: Let's break it down**\n\n- Become one with the data (visualize, visualize, visualize...)\n- Preprocess the data (prepare it for a model)\n    - A batch is a small subset of the dataset a model looks at during training. \n        For example, rather than looking at 10,000 images at one time and trying to figure out the patterns, a model might only look at 32 images at a time.\n        It does this for a couple of reasons:\n        - 10,000 images (or more) might not fit into the memory of your processor (GPU).\n        - Trying to learn the patterns in 10,000 images in one hit could result in the model not being able to learn very well.\n        - Why 32? A batch size of 32 is good for your health.There are many different batch sizes you could use but 32 has proven to be very effective in many different use cases and is often the default for many data preprocessing functions.\n        - The ImageDataGenerator class helps us prepare our images into batches as well as perform transformations on them as they get loaded into the model.You might've noticed the rescale parameter. This is one example of the transformations we're doing.\n- Create a model (start with a baseline)\n    - And it follows the typical CNN structure of:\n        \n      Input -> Conv + ReLU layers (non-linearities) -> Pooling layer -> Fully connected (dense layer) as Output\n    - Let's discuss some of the components of the Conv2D layer:\n      - The \"2D\" means our inputs are two dimensional (height and width), even though they have 3 colour channels, the convolutions are run on each channel invididually.\n      - filters - these are the number of \"feature extractors\" that will be moving over our images.\n      - kernel_size - the size of our filters, for example, a kernel_size of (3, 3) (or just 3) will mean each filter will have the size 3x3, meaning it will look at a space of 3x3 pixels each time. The smaller the kernel, the more fine-grained features it will extract.\n      - stride - the number of pixels a filter will move across as it covers the image. A stride of 1 means the filter moves across each pixel 1 by 1. A stride of 2 means it moves 2 pixels at a time.\n      - padding - this can be either 'same' or 'valid', 'same' adds zeros the to outside of the image so the resulting output of the convolutional layer is the same as the input, where as 'valid' (default) cuts off excess pixels where the filter doesn't fit (e.g. 224 pixels wide divided by a kernel size of 3 (224/3 = 74.6) means a single pixel will get cut off the end.\n   - Since we're working on a binary classification problem (pizza vs. steak), the loss function we're using is 'binary_crossentropy', if it was mult-iclass, we might use something like 'categorical_crossentropy'.\n   - Adam with all the default settings is our optimizer and our evaluation metric is accuracy.\n- Fit the model\n- Evaluate the model\n   - When a model's validation loss starts to increase, it's likely that it's overfitting the training dataset. This means, it's learning the patterns in the training dataset too well and thus its ability to generalize to unseen data will be diminished.\n- Adjust different parameters and improve model (try to beat your baseline)\n   - Fitting a machine learning model comes in 3 steps:\n       - Create a basline.\n       - Beat the baseline by overfitting a larger model.\n       - Reduce overfitting.\n   - And there are even a few more things we could try to further overfit our model:\n       - Increase the number of convolutional layers.\n       - Increase the number of convolutional filters.\n       - Add another dense layer to the output of our flattened layer.\n- Repeat until satisfied\n","metadata":{}},{"cell_type":"markdown","source":"Lets build 2 models \n - A convnet with max pooling \n - a convnet with max pooling and data augmentation\n \nFor the first model, we'll follow the modified basic CNN structure:\n - Input -> Conv layers + ReLU layers (non-linearities) + Max Pooling layers -> Fully connected (dense layer) as Output","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D, Dense,Flatten","metadata":{"execution":{"iopub.status.busy":"2021-11-09T13:49:25.831650Z","iopub.execute_input":"2021-11-09T13:49:25.831862Z","iopub.status.idle":"2021-11-09T13:49:25.838510Z","shell.execute_reply.started":"2021-11-09T13:49:25.831836Z","shell.execute_reply":"2021-11-09T13:49:25.837562Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"model_2 = Sequential()\nmodel_2.add(Conv2D(10,(3,3),activation='relu',input_shape=(224,224,3)))\nmodel_2.add(MaxPool2D((2,2))) # reduce the number of features by half\nmodel_2.add(Conv2D(10,(3,3),activation=\"relu\"))\nmodel_2.add(MaxPool2D((2,2)))\nmodel_2.add(Conv2D(10,(3,3),activation=\"relu\"))\nmodel_2.add(MaxPool2D((2,2)))\nmodel_2.add(Flatten())\nmodel_2.add(Dense(1,activation =\"sigmoid\"))\nmodel_2.compile(loss ='binary_crossentropy',\n               optimizer = tf.keras.optimizers.Adam(),\n               metrics = ['accuracy'])\nhist_2 =model_2.fit(train_data,\n                   epochs = 5,\n                   steps_per_epoch = len(train_data),\n                   validation_data = valid_data,\n                   validation_steps = len(valid_data))","metadata":{"execution":{"iopub.status.busy":"2021-11-09T13:49:25.842061Z","iopub.execute_input":"2021-11-09T13:49:25.843166Z","iopub.status.idle":"2021-11-09T13:50:13.463793Z","shell.execute_reply.started":"2021-11-09T13:49:25.843120Z","shell.execute_reply":"2021-11-09T13:50:13.462966Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"model_2.summary()","metadata":{"execution":{"iopub.status.busy":"2021-11-09T13:50:13.465413Z","iopub.execute_input":"2021-11-09T13:50:13.465929Z","iopub.status.idle":"2021-11-09T13:50:13.475781Z","shell.execute_reply.started":"2021-11-09T13:50:13.465889Z","shell.execute_reply":"2021-11-09T13:50:13.474868Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def plot_loss_curves(history):\n    \"\"\"\n    returns seperate accuracy and loss curves\n    \"\"\"\n    loss = history.history['loss']\n    val_loss =history.history['val_loss']\n    \n    acc  = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    \n    epochs= range(len(history.history['loss']))\n    \n    plt.plot(epochs,loss,label='training_loss')\n    plt.plot(epochs,val_loss,label='val_loss')\n    plt.title('loss')\n    plt.xlabel('Epochs')\n    plt.legend()\n    \n    plt.figure()\n    plt.plot(epochs,acc,label ='training accuracy')\n    plt.plot(epochs,val_acc,label='val_accuracy')\n    plt.title('Accuracy')\n    plt.xlabel('Epochs')\n    plt.legend();","metadata":{"execution":{"iopub.status.busy":"2021-11-09T13:50:13.477070Z","iopub.execute_input":"2021-11-09T13:50:13.477415Z","iopub.status.idle":"2021-11-09T13:50:13.486635Z","shell.execute_reply.started":"2021-11-09T13:50:13.477372Z","shell.execute_reply":"2021-11-09T13:50:13.485725Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"plot_loss_curves(hist_2)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T13:50:13.488529Z","iopub.execute_input":"2021-11-09T13:50:13.489159Z","iopub.status.idle":"2021-11-09T13:50:14.063297Z","shell.execute_reply.started":"2021-11-09T13:50:13.489118Z","shell.execute_reply":"2021-11-09T13:50:14.062329Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"The validation loss looks to start increasing towards the end and in turn potentially leading to overfitting.Time to dig into our bag of tricks and try another method of overfitting prevention, data augmentation.To implement data augmentation, we'll have to reinstantiate our ImageDataGenerator instances.","metadata":{}},{"cell_type":"code","source":"train_datagen_augmented = ImageDataGenerator(rescale =1.0/255,\n                                            rotation_range =20,\n                                            shear_range = 0.2,\n                                            zoom_range =0.2,\n                                            width_shift_range =0.2,\n                                            height_shift_range =0.2,\n                                            horizontal_flip =True)\ntest_datagen = ImageDataGenerator(rescale=1.0/255)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T13:50:14.065156Z","iopub.execute_input":"2021-11-09T13:50:14.065486Z","iopub.status.idle":"2021-11-09T13:50:14.072580Z","shell.execute_reply.started":"2021-11-09T13:50:14.065428Z","shell.execute_reply":"2021-11-09T13:50:14.071120Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"train_data_aug = train_datagen_augmented.flow_from_directory(train_dir,\n                                                            target_size =(224,224),\n                                                             batch_size = 32,\n                                                             class_mode = 'binary',\n                                                             shuffle = False)\ntest_data = test_datagen.flow_from_directory(test_dir,\n                                            target_size=(224,224),\n                                            batch_size =32,\n                                            class_mode ='binary')","metadata":{"execution":{"iopub.status.busy":"2021-11-09T13:50:14.074707Z","iopub.execute_input":"2021-11-09T13:50:14.075299Z","iopub.status.idle":"2021-11-09T13:50:14.293580Z","shell.execute_reply.started":"2021-11-09T13:50:14.075252Z","shell.execute_reply":"2021-11-09T13:50:14.291851Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"model_3 = Sequential([\n    Conv2D(10,3,activation ='relu',input_shape=(224,224,3)),\n    MaxPool2D(pool_size=2),\n    Conv2D(10,3,activation='relu'),\n    MaxPool2D(),\n    Conv2D(10,3,activation='relu'),\n    MaxPool2D(),\n    Flatten(),\n    Dense(1,activation='sigmoid')\n])\nmodel_3.compile(loss ='binary_crossentropy',\n               optimizer =tf.keras.optimizers.Adam(),\n               metrics =['accuracy'])\nhist_3 = model_3.fit(train_data_aug,\n                    epochs =5,\n                    steps_per_epoch =len(train_data_aug),\n                    validation_data = valid_data,\n                    validation_steps = len(valid_data))","metadata":{"execution":{"iopub.status.busy":"2021-11-09T13:50:14.294966Z","iopub.execute_input":"2021-11-09T13:50:14.295245Z","iopub.status.idle":"2021-11-09T13:52:39.693751Z","shell.execute_reply.started":"2021-11-09T13:50:14.295205Z","shell.execute_reply":"2021-11-09T13:52:39.693011Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"when we created train_data_augmented we turned off data shuffling using shuffle=False which means our model only sees a batch of a single kind of images at a time.\n\nFor example, the pizza class gets loaded in first because it's the first class. Thus it's performance is measured on only a single class rather than both classes. The validation data performance improves steadily because it contains shuffled data.\n\nSince we only set shuffle=False for demonstration purposes (so we could plot the same augmented and non-augmented image), we can fix this by setting shuffle=True on future data generators.\n\nImageDataGenerator instance augments the data as it's loaded into the model. The benefit of this is that it leaves the original images unchanged. The downside is that it takes longer to load them in.\n\n>One possible method to speed up dataset manipulation would be to look into TensorFlow's parrallel reads and buffered prefecting options.","metadata":{}},{"cell_type":"code","source":"plot_loss_curves(hist_3)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T13:52:39.696793Z","iopub.execute_input":"2021-11-09T13:52:39.697013Z","iopub.status.idle":"2021-11-09T13:52:40.163165Z","shell.execute_reply.started":"2021-11-09T13:52:39.696987Z","shell.execute_reply":"2021-11-09T13:52:40.162476Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"train_data_aug = train_datagen_augmented.flow_from_directory(train_dir,\n                                                            target_size =(224,224),\n                                                             batch_size = 32,\n                                                             class_mode = 'binary',\n                                                             shuffle = True)\ntest_data = test_datagen.flow_from_directory(test_dir,\n                                            target_size=(224,224),\n                                            batch_size =32,\n                                            class_mode ='binary')","metadata":{"execution":{"iopub.status.busy":"2021-11-09T13:52:40.164536Z","iopub.execute_input":"2021-11-09T13:52:40.164778Z","iopub.status.idle":"2021-11-09T13:52:40.378502Z","shell.execute_reply.started":"2021-11-09T13:52:40.164743Z","shell.execute_reply":"2021-11-09T13:52:40.377704Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"model_4 = Sequential([\n    Conv2D(10,3,activation ='relu',input_shape=(224,224,3)),\n    MaxPool2D(pool_size=2),\n    Conv2D(10,3,activation='relu'),\n    MaxPool2D(),\n    Conv2D(10,3,activation='relu'),\n    MaxPool2D(),\n    Flatten(),\n    Dense(1,activation='sigmoid')\n])\nmodel_4.compile(loss ='binary_crossentropy',\n               optimizer =tf.keras.optimizers.Adam(),\n               metrics =['accuracy'])\nhist_4 = model_4.fit(train_data_aug,\n                    epochs =5,\n                    steps_per_epoch =len(train_data_aug),\n                    validation_data = valid_data,\n                    validation_steps = len(valid_data))","metadata":{"execution":{"iopub.status.busy":"2021-11-09T13:52:40.379528Z","iopub.execute_input":"2021-11-09T13:52:40.379968Z","iopub.status.idle":"2021-11-09T13:55:03.373146Z","shell.execute_reply.started":"2021-11-09T13:52:40.379927Z","shell.execute_reply":"2021-11-09T13:55:03.372424Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"plot_loss_curves(hist_4)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T13:55:03.374553Z","iopub.execute_input":"2021-11-09T13:55:03.374810Z","iopub.status.idle":"2021-11-09T13:55:03.812061Z","shell.execute_reply.started":"2021-11-09T13:55:03.374775Z","shell.execute_reply":"2021-11-09T13:55:03.811316Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"#### Repeat until satisified\nWe've trained a few model's on our dataset already and so far they're performing pretty good, we could try to continue to improve our model:\n- Increase the number of model layers (e.g. add more convolutional layers).\n- Increase the number of filters in each convolutional layer (e.g. from 10 to 32, 64, or 128, these numbers aren't set in stone either, they are usually found through trial and error).\n- Train for longer (more epochs).\n- Finding an ideal learning rate.\n- Get more data (give the model more opportunities to learn).\n- Use transfer learning to leverage what another image model has learned and adjust it for our own use case.\n- Adjusting each of these settings (except for the last two) during model development is usually referred to as hyperparameter tuning.\n\nYou can think of hyperparameter tuning as similar to adjusting the settings on your oven to cook your favourite dish. Although your oven does most of the cooking for you, you can help it by tweaking the dials.","metadata":{}},{"cell_type":"code","source":"model_5 = Sequential([\n    Conv2D(32,(3,3),activation='relu',input_shape=(224,224,3)),\n#     Conv2D(64,(3,3),activation='relu'),\n    MaxPool2D((2,2)),\n#     Conv2D(32,(3,3),activation='relu'),\n#     Conv2D(32,(3,3),activation='relu'),\n#     MaxPool2D((2,2)),\n    Conv2D(16,(3,3),activation='relu'),\n    MaxPool2D((2,2)),\n    Flatten(),\n#     Dense(32,activation= 'relu'),\n    Dense(1,activation ='sigmoid')\n])\n\nmodel_5.compile(loss ='binary_crossentropy',\n               optimizer = tf.keras.optimizers.Adam(learning_rate =0.001),\n               metrics =['accuracy'])\nhist_5 = model_5.fit(train_data_aug,\n                    epochs =14,\n                    steps_per_epoch =len(train_data_aug),\n                    validation_data=test_data,\n                    validation_steps = len(test_data))","metadata":{"execution":{"iopub.status.busy":"2021-11-09T13:55:03.813369Z","iopub.execute_input":"2021-11-09T13:55:03.813641Z","iopub.status.idle":"2021-11-09T14:00:58.338508Z","shell.execute_reply.started":"2021-11-09T13:55:03.813605Z","shell.execute_reply":"2021-11-09T14:00:58.337797Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# View our example image\nimport matplotlib.image as mpimg \n!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-steak.jpeg \nsteak = mpimg.imread(\"03-steak.jpeg\")\nplt.imshow(steak)\nplt.axis(False);","metadata":{"execution":{"iopub.status.busy":"2021-11-09T14:00:58.340006Z","iopub.execute_input":"2021-11-09T14:00:58.340268Z","iopub.status.idle":"2021-11-09T14:01:01.214349Z","shell.execute_reply.started":"2021-11-09T14:00:58.340233Z","shell.execute_reply":"2021-11-09T14:01:01.213614Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"steak.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-09T14:01:01.215784Z","iopub.execute_input":"2021-11-09T14:01:01.216618Z","iopub.status.idle":"2021-11-09T14:01:01.222717Z","shell.execute_reply.started":"2021-11-09T14:01:01.216575Z","shell.execute_reply":"2021-11-09T14:01:01.221895Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Since our model takes in images of shapes (224, 224, 3), we've got to reshape our custom image to use it with our model.\ndef pre_proc(filename,img_shape=224):\n    \"\"\"\n    reads a image from file and converts it to tensor and reshapes it\"\"\"\n    img = tf.io.read_file(filename)\n    img = tf.image.decode_image(img,channels=3)\n    img = tf.image.resize(img,size =[img_shape,img_shape])\n    img =img/255.\n    return img","metadata":{"execution":{"iopub.status.busy":"2021-11-09T14:01:01.224671Z","iopub.execute_input":"2021-11-09T14:01:01.224990Z","iopub.status.idle":"2021-11-09T14:01:01.232165Z","shell.execute_reply.started":"2021-11-09T14:01:01.224939Z","shell.execute_reply":"2021-11-09T14:01:01.231308Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"steak = pre_proc(\"../working/03-steak.jpeg\")\nsteak.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-09T14:01:01.233166Z","iopub.execute_input":"2021-11-09T14:01:01.234645Z","iopub.status.idle":"2021-11-09T14:01:01.368187Z","shell.execute_reply.started":"2021-11-09T14:01:01.234559Z","shell.execute_reply":"2021-11-09T14:01:01.367395Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"model_5.predict(steak)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T14:01:01.369801Z","iopub.execute_input":"2021-11-09T14:01:01.370269Z","iopub.status.idle":"2021-11-09T14:01:01.768636Z","shell.execute_reply.started":"2021-11-09T14:01:01.370225Z","shell.execute_reply":"2021-11-09T14:01:01.766344Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"\nThere's one more problem...\n\nAlthough our image is in the same shape as the images our model has been trained on, we're still missing a dimension.\n\nRemember how our model was trained in batches?\n\nWell, the batch size becomes the first dimension.\n\nSo in reality, our model was trained on data in the shape of (batch_size, 224, 224, 3).\n\nWe can fix this by adding an extra to our custom image tensor using tf.expand_dims.","metadata":{}},{"cell_type":"code","source":"#  Add an extra axis\nprint(f\"Shape before new dimension: {steak.shape}\")\nsteak = tf.expand_dims(steak, axis=0) # add an extra dimension at axis 0\n#steak = steak[tf.newaxis, ...] # alternative to the above, '...' is short for 'every other dimension'\nprint(f\"Shape after new dimension: {steak.shape}\")\nsteak\n","metadata":{"execution":{"iopub.status.busy":"2021-11-09T14:01:58.245739Z","iopub.execute_input":"2021-11-09T14:01:58.246006Z","iopub.status.idle":"2021-11-09T14:01:58.259391Z","shell.execute_reply.started":"2021-11-09T14:01:58.245976Z","shell.execute_reply":"2021-11-09T14:01:58.258353Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"pred = model_5.predict(steak)\npred","metadata":{"execution":{"iopub.status.busy":"2021-11-09T14:01:58.681059Z","iopub.execute_input":"2021-11-09T14:01:58.681320Z","iopub.status.idle":"2021-11-09T14:01:58.791041Z","shell.execute_reply.started":"2021-11-09T14:01:58.681289Z","shell.execute_reply":"2021-11-09T14:01:58.790327Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"Ahh, the predictions come out in prediction probability form. In other words, this means how likely the image is to be one class or another.\n\nSince we're working with a binary classification problem, if the prediction probability is over 0.5, according to the model, the prediction is most likely to be the postive class (class 1).\n\nAnd if the prediction probability is under 0.5, according to the model, the predicted class is most likely to be the negative class (class 0).","metadata":{}},{"cell_type":"code","source":"class_names =['pizza','steak']","metadata":{"execution":{"iopub.status.busy":"2021-11-09T14:01:59.253257Z","iopub.execute_input":"2021-11-09T14:01:59.253542Z","iopub.status.idle":"2021-11-09T14:01:59.258209Z","shell.execute_reply.started":"2021-11-09T14:01:59.253508Z","shell.execute_reply":"2021-11-09T14:01:59.257396Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"pred_class = class_names[int(tf.round(pred)[0][0])]\npred_class","metadata":{"execution":{"iopub.status.busy":"2021-11-09T14:01:59.509114Z","iopub.execute_input":"2021-11-09T14:01:59.509740Z","iopub.status.idle":"2021-11-09T14:01:59.519411Z","shell.execute_reply.started":"2021-11-09T14:01:59.509706Z","shell.execute_reply":"2021-11-09T14:01:59.518564Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"def pred_and_plot(model,filename,class_names):\n    \"\"\"imports an image located at filename, makes a prediction on it  with a trained model\n    and plots the image with predicted class as the title\"\"\"\n    img = pre_proc(filename)\n    pred = model.predict(tf.expand_dims(img,axis =0))\n    pred_class = class_names[int(tf.round(pred)[0][0])]\n    plt.imshow(img)\n    plt.title(f\"prediction: {pred_class}\")\n    plt.axis(False)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T14:01:59.848348Z","iopub.execute_input":"2021-11-09T14:01:59.848873Z","iopub.status.idle":"2021-11-09T14:01:59.854114Z","shell.execute_reply.started":"2021-11-09T14:01:59.848836Z","shell.execute_reply":"2021-11-09T14:01:59.853211Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"pred_and_plot(model_5,\"./03-steak.jpeg\",class_names)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T14:02:00.263785Z","iopub.execute_input":"2021-11-09T14:02:00.264045Z","iopub.status.idle":"2021-11-09T14:02:00.562118Z","shell.execute_reply.started":"2021-11-09T14:02:00.264015Z","shell.execute_reply":"2021-11-09T14:02:00.561269Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"\n# Download another test image and make a prediction on it\n!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-pizza-dad.jpeg \npred_and_plot(model_5, \"./03-pizza-dad.jpeg\", class_names)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-09T14:02:00.563760Z","iopub.execute_input":"2021-11-09T14:02:00.564125Z","iopub.status.idle":"2021-11-09T14:02:01.871227Z","shell.execute_reply.started":"2021-11-09T14:02:00.564084Z","shell.execute_reply":"2021-11-09T14:02:01.870406Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"## Multi-class Verification\n### import and become one with the data\n","metadata":{}},{"cell_type":"code","source":"\nimport zipfile\n\n# Download zip file of 10_food_classes images\n# See how this data was created - https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/extras/image_data_modification.ipynb\n!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_all_data.zip \n\n# Unzip the downloaded file\nzip_ref = zipfile.ZipFile(\"10_food_classes_all_data.zip\", \"r\")\nzip_ref.extractall()\nzip_ref.close() ","metadata":{"execution":{"iopub.status.busy":"2021-11-09T14:02:01.873567Z","iopub.execute_input":"2021-11-09T14:02:01.873850Z","iopub.status.idle":"2021-11-09T14:02:10.646124Z","shell.execute_reply.started":"2021-11-09T14:02:01.873809Z","shell.execute_reply":"2021-11-09T14:02:10.645284Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"import os\n# Walk through 10_food_classes directory and list number of files\nfor dirpath, dirnames, filenames in os.walk(\"10_food_classes_all_data\"):\n  print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")","metadata":{"execution":{"iopub.status.busy":"2021-11-09T14:02:10.647764Z","iopub.execute_input":"2021-11-09T14:02:10.648059Z","iopub.status.idle":"2021-11-09T14:02:10.674408Z","shell.execute_reply.started":"2021-11-09T14:02:10.648019Z","shell.execute_reply":"2021-11-09T14:02:10.673530Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"train_dir = \"10_food_classes_all_data/train/\"\ntest_dir = \"10_food_classes_all_data/test/\"\nimport pathlib\nimport numpy as np\ndata_dir = pathlib.Path(train_dir)\nclass_names = np.array(sorted([item.name  for item in data_dir.glob('*')]))\nclass_names","metadata":{"execution":{"iopub.status.busy":"2021-11-09T14:02:10.676419Z","iopub.execute_input":"2021-11-09T14:02:10.676765Z","iopub.status.idle":"2021-11-09T14:02:10.684578Z","shell.execute_reply.started":"2021-11-09T14:02:10.676726Z","shell.execute_reply":"2021-11-09T14:02:10.683820Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen_aug = ImageDataGenerator(rescale =1/255.,\n                                   shear_range =0.2,\n                                   zoom_range = 0.2,\n                                   rotation_range =20,\n                                   width_shift_range=0.2,\n                                   height_shift_range = 0.2,\n                                   horizontal_flip = True)\ntest_datagen = ImageDataGenerator(rescale=1/255.)\n\ntrain_data = train_datagen_aug.flow_from_directory(train_dir,\n                                                  target_size=(224,224),\n                                                  batch_size =32,\n                                                  class_mode ='categorical')\ntest_data = test_datagen.flow_from_directory(test_dir,\n                                            target_size=(224,224),\n                                             batch_size =32,\n                                             class_mode ='categorical')","metadata":{"execution":{"iopub.status.busy":"2021-11-09T14:02:10.685918Z","iopub.execute_input":"2021-11-09T14:02:10.686395Z","iopub.status.idle":"2021-11-09T14:02:11.325880Z","shell.execute_reply.started":"2021-11-09T14:02:10.686356Z","shell.execute_reply":"2021-11-09T14:02:11.325141Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"model_6 = Sequential([\n    Conv2D(16,(5,5),activation='relu',input_shape=(224,224,3)),\n#     Conv2D(64,(5,5),activation='relu'),\n#     Conv2D(64,(3,3),activation='relu'),\n    MaxPool2D((2,2)),\n#     Conv2D(64,(3,3),activation='relu'),\n#     Conv2D(64,(3,3),activation='relu'),\n    Conv2D(8,(3,3),activation='relu'),\n    MaxPool2D(),\n    Flatten(),\n#     Dense(10,activation='relu'),\n    Dense(10,activation='softmax')\n])\n\nmodel_6.compile(loss='categorical_crossentropy',\n               optimizer = tf.keras.optimizers.Adam(),\n               metrics=['accuracy'])\nhist_6 = model_6.fit(train_data,\n                    epochs =10,\n                    steps_per_epoch= len(train_data),\n                    validation_data = test_data,\n                    validation_steps = len(test_data))","metadata":{"execution":{"iopub.status.busy":"2021-11-09T14:49:31.947325Z","iopub.execute_input":"2021-11-09T14:49:31.948101Z","iopub.status.idle":"2021-11-09T15:10:19.890226Z","shell.execute_reply.started":"2021-11-09T14:49:31.948063Z","shell.execute_reply":"2021-11-09T15:10:19.889492Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"Now we've got augmented data, let's see how it works with the same model as before (model_10).\n\nRather than rewrite the model from scratch, we can clone it using a handy function in TensorFlow called clone_model which can take an existing model and rebuild it in the same format.\n\nThe cloned version will not include any of the weights (patterns) the original model has learned. So when we train it, it'll be like training a model from scratch.\n\nðŸ”‘ Note: One of the key practices in deep learning and machine learning in general is to be a serial experimenter. That's what we're doing here. Trying something, seeing if it works, then trying something else. A good experiment setup also keeps track of the things you change, for example, that's why we're using the same model as before but with different data. The model stays the same but the data changes, this will let us know if augmented training data has any influence over performance.","metadata":{}},{"cell_type":"code","source":"# -q is for \"quiet\"\n!wget -q https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-pizza-dad.jpeg\n!wget -q https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-steak.jpeg\n!wget -q https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-hamburger.jpeg\n!wget -q https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-sushi.jpeg","metadata":{"execution":{"iopub.status.busy":"2021-11-09T15:10:19.892103Z","iopub.execute_input":"2021-11-09T15:10:19.892812Z","iopub.status.idle":"2021-11-09T15:10:25.002896Z","shell.execute_reply.started":"2021-11-09T15:10:19.892770Z","shell.execute_reply":"2021-11-09T15:10:25.001889Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"# Adjust function to work with multi-class\ndef pred_and_plot(model, filename, class_names):\n    \"\"\"\n    Imports an image located at filename, makes a prediction on it with\n    a trained model and plots the image with the predicted class as the title.\n    \"\"\"\n    img = pre_proc(filename)\n    pred = model.predict(tf.expand_dims(img, axis=0))\n    if len(pred[0]) > 1: # check for multi-class\n        pred_class = class_names[pred.argmax()] # if more than one output, take the max\n    else:\n        pred_class = class_names[int(tf.round(pred)[0][0])] # if only one output, round\n    plt.imshow(img)\n    plt.title(f\"Prediction: {pred_class}\")\n    plt.axis(False);","metadata":{"execution":{"iopub.status.busy":"2021-11-09T15:11:18.918319Z","iopub.execute_input":"2021-11-09T15:11:18.918970Z","iopub.status.idle":"2021-11-09T15:11:18.926328Z","shell.execute_reply.started":"2021-11-09T15:11:18.918926Z","shell.execute_reply":"2021-11-09T15:11:18.925176Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"pred_and_plot(model_6, \"03-pizza-dad.jpeg\", class_names)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T15:11:22.294550Z","iopub.execute_input":"2021-11-09T15:11:22.294955Z","iopub.status.idle":"2021-11-09T15:11:22.630703Z","shell.execute_reply.started":"2021-11-09T15:11:22.294916Z","shell.execute_reply":"2021-11-09T15:11:22.629925Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"pred_and_plot(model_6, \"03-sushi.jpeg\", class_names)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T15:11:37.928069Z","iopub.execute_input":"2021-11-09T15:11:37.928322Z","iopub.status.idle":"2021-11-09T15:11:38.243911Z","shell.execute_reply.started":"2021-11-09T15:11:37.928293Z","shell.execute_reply":"2021-11-09T15:11:38.243073Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"pred_and_plot(model_6, \"03-hamburger.jpeg\", class_names)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T15:11:58.629812Z","iopub.execute_input":"2021-11-09T15:11:58.630233Z","iopub.status.idle":"2021-11-09T15:11:59.071751Z","shell.execute_reply.started":"2021-11-09T15:11:58.630196Z","shell.execute_reply":"2021-11-09T15:11:59.070962Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"pred_and_plot(model_6, \"03-steak.jpeg\", class_names)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T15:12:18.286377Z","iopub.execute_input":"2021-11-09T15:12:18.287104Z","iopub.status.idle":"2021-11-09T15:12:18.611239Z","shell.execute_reply.started":"2021-11-09T15:12:18.287066Z","shell.execute_reply":"2021-11-09T15:12:18.610513Z"},"trusted":true},"execution_count":56,"outputs":[]}]}